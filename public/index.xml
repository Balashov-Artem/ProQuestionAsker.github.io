<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amber Thomas</title>
    <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/index.xml</link>
    <description>Recent content on Amber Thomas</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy;2016 Amber Thomas</copyright>
    <lastBuildDate>Wed, 16 Nov 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dog Ownership in Seattle</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Seattle_Dogs/</link>
      <pubDate>Wed, 16 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Seattle_Dogs/</guid>
      <description>&lt;p&gt;Data exploration, mapping, and data viz in RMarkdown.
&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This report investigates licensed dog ownership in Seattle, WA (USA).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m curious about a few things here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;People estimate that there are 160,000 dogs in Seattle. Where are they?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Seattle is a relatively densely-populated area. Are small, apartment-friendly dogs preferred?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Using this information, what recommendations could be made to aspiring dog sitters and walkers in Seattle?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will annotate each step of data analysis as I go.&lt;/p&gt;

&lt;p&gt;Time to get started!&lt;/p&gt;

&lt;h3 id=&#34;loading-necessary-packages&#34;&gt;Loading Necessary Packages&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# For mapping
library(choroplethr)
library(choroplethrZip)
library(ggmap)
library(mapproj)
library(zipcode)

# For data manipulation and tidying
library(dplyr)
library(tidyr)

# For data visualizations
library(ggplot2)
library(tm)
library(SnowballC)
library(wordcloud)

# For modeling and machine learning
library(caret)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;importing-licensed-dog-ownership-data&#34;&gt;Importing Licensed Dog Ownership Data&lt;/h3&gt;

&lt;p&gt;The spreadsheet containing all licensed dog ownership information was obtained from the Seattle Times article &lt;a href=&#34;http://www.seattletimes.com/life/pets/mapping-the-dogs-of-seattle/&#34;&gt;&amp;ldquo;Mapping the Dogs of Seattle&amp;rdquo;&lt;/a&gt; and is available for &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1XWLw_hxWM2RHiwALzcM_QxNpzQn_cDmS3Z9HFoZMvTo/edit#gid=460106206&#34;&gt;download&lt;/a&gt;. According to the article, the original data were obtained from the Seattle Animal Shelter and represent the 43,000 licensed dogs in Seattle as of February 2015. &lt;em&gt;Note: This number is thought to only represent approximately &lt;a href=&#34;http://www.seattle.gov/Documents/Departments/ParksAndRecreation/PoliciesPlanning/Plans/Response_to_SLI_69-1-B-1_(Dog_Off-Leash_Areas).pdf&#34;&gt;27% of the dog population in Seattle&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs &amp;lt;- read.csv(file = &amp;quot;Seattle_Dogs_2015.csv&amp;quot;, header = TRUE, 
    stringsAsFactors = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Let&amp;rsquo;s take a quick look at the data file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(dogs)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:    42996 obs. of  7 variables:
##  $ License.Type.Sold: Factor w/ 15 levels &amp;quot;Dog 6 Month Prov/Rabies&amp;quot;,..: 1 1 1 1 1 1 1 1 2 2 ...
##  $ Animal.Type      : Factor w/ 1 level &amp;quot;Dog&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Gender           : Factor w/ 3 levels &amp;quot;Female&amp;quot;,&amp;quot;Male&amp;quot;,..: 2 1 1 2 2 1 2 2 1 2 ...
##  $ Primary.Breed    : Factor w/ 227 levels &amp;quot;Affenpinscher&amp;quot;,..: 53 53 174 123 137 102 53 106 26 15 ...
##  $ Primary.Color    : Factor w/ 90 levels &amp;quot;&amp;quot;,&amp;quot;Amber&amp;quot;,&amp;quot;Apricot&amp;quot;,..: 85 6 85 10 10 9 9 36 83 56 ...
##  $ Name             : Factor w/ 10712 levels &amp;quot;&amp;quot;,&amp;quot; &amp;quot;,&amp;quot; Carlota&amp;quot;,..: 603 9763 317 1324 3671 5764 6065 4254 490 7006 ...
##  $ Zip.C            : Factor w/ 155 levels &amp;quot;&amp;quot;,&amp;quot;*/116&amp;quot;,&amp;quot;14534&amp;quot;,..: 107 107 100 75 84 1 86 77 107 87 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the variables we are working with right now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;License Type&lt;/strong&gt; : Indicates what type of license the dog has&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Animal Type&lt;/strong&gt; : Since this dataset is all about dogs, there is only one animal type listed: &amp;ldquo;Dog&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gender&lt;/strong&gt; : Dog&amp;rsquo;s sex (&amp;ldquo;Male&amp;rdquo;, &amp;ldquo;Female&amp;rdquo;, or &amp;ldquo;Unspec&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Primary Breed&lt;/strong&gt; : Indicates the general breed of the dog&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Primary Color&lt;/strong&gt; : Indicates the dog&amp;rsquo;s overall color&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; : Lists the dog&amp;rsquo;s name&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zip.C&lt;/strong&gt; : Indicates the zipcode where the dog is registered&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can eliminate the &amp;ldquo;Animal Type&amp;rdquo; column since it doesn&amp;rsquo;t give us any additional information.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs &amp;lt;- dogs %&amp;gt;% select(-2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, now is there anyway to condense or standardize the breeds listed?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at what kind of breeds are present in Seattle.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;levels(dogs$Primary.Breed)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;Affenpinscher&amp;quot;                  &amp;quot;Afghan Hound&amp;quot;                  
##   [3] &amp;quot;Airedale Terrier&amp;quot;               &amp;quot;Akita&amp;quot;                         
##   [5] &amp;quot;Alaskan Malumute&amp;quot;               &amp;quot;Amer. Pitbull Terrier&amp;quot;         
##   [7] &amp;quot;Amer. Water Spaniel&amp;quot;            &amp;quot;Amer.Staffordshire Terrier&amp;quot;    
##   [9] &amp;quot;American Eskimo&amp;quot;                &amp;quot;American Foxhound&amp;quot;             
##  [11] &amp;quot;Appenzel Mountain Dog&amp;quot;          &amp;quot;Australian Cattle Dog&amp;quot;         
##  [13] &amp;quot;Australian Kelpie&amp;quot;              &amp;quot;Australian Shepard&amp;quot;            
##  [15] &amp;quot;Australian Terrier&amp;quot;             &amp;quot;Basenji&amp;quot;                       
##  [17] &amp;quot;Basset Hound&amp;quot;                   &amp;quot;Beagle&amp;quot;                        
##  [19] &amp;quot;Bearded Collie&amp;quot;                 &amp;quot;Beauceron&amp;quot;                     
##  [21] &amp;quot;Belgian Malinios&amp;quot;               &amp;quot;Belgian Malinois&amp;quot;              
##  [23] &amp;quot;Belgian Sheepdog&amp;quot;               &amp;quot;Belgian Shepherd&amp;quot;              
##  [25] &amp;quot;Belgian Tervuren&amp;quot;               &amp;quot;Bernese Mountain Dog&amp;quot;          
##  [27] &amp;quot;Bichon Frise&amp;quot;                   &amp;quot;Bloodhound&amp;quot;                    
##  [29] &amp;quot;Blue Heeler&amp;quot;                    &amp;quot;Bordeaux Mastiff&amp;quot;              
##  [31] &amp;quot;Border Collie&amp;quot;                  &amp;quot;Border Terrier&amp;quot;                
##  [33] &amp;quot;Borzoi&amp;quot;                         &amp;quot;Boston Terrier&amp;quot;                
##  [35] &amp;quot;Bouvier De Flanders&amp;quot;            &amp;quot;Boxer&amp;quot;                         
##  [37] &amp;quot;Briard&amp;quot;                         &amp;quot;Brittany Spaniel&amp;quot;              
##  [39] &amp;quot;Brussels Griffon&amp;quot;               &amp;quot;Bull Terrier&amp;quot;                  
##  [41] &amp;quot;Bull Terrier, Minature&amp;quot;         &amp;quot;Bulldog&amp;quot;                       
##  [43] &amp;quot;Bulldog American&amp;quot;               &amp;quot;Bulldog English&amp;quot;               
##  [45] &amp;quot;Bullmastiff&amp;quot;                    &amp;quot;Cairn Terrier&amp;quot;                 
##  [47] &amp;quot;Canaan Dog&amp;quot;                     &amp;quot;Cane Corso&amp;quot;                    
##  [49] &amp;quot;Carolina&amp;quot;                       &amp;quot;Catahoula Leopard Dog&amp;quot;         
##  [51] &amp;quot;Cavalier King Charles Spaniel&amp;quot;  &amp;quot;Chesapeake Bay Retriever&amp;quot;      
##  [53] &amp;quot;Chihuahua&amp;quot;                      &amp;quot;Chihuahua, Long-haired&amp;quot;        
##  [55] &amp;quot;Chinese Crested&amp;quot;                &amp;quot;Chinook&amp;quot;                       
##  [57] &amp;quot;Chow Chow&amp;quot;                      &amp;quot;Cirneco Dell Etna&amp;quot;             
##  [59] &amp;quot;Clumber Spaniel&amp;quot;                &amp;quot;Cocker Spaniel&amp;quot;                
##  [61] &amp;quot;Cocker Spaniel, American&amp;quot;       &amp;quot;Cocker Spaniel, English&amp;quot;       
##  [63] &amp;quot;Collie&amp;quot;                         &amp;quot;Collie, Rough-Coated&amp;quot;          
##  [65] &amp;quot;Collie, Smooth-Coated&amp;quot;          &amp;quot;Coonhound&amp;quot;                     
##  [67] &amp;quot;Coonhound, Redbone&amp;quot;             &amp;quot;Coonhound, Walker&amp;quot;             
##  [69] &amp;quot;Corgi&amp;quot;                          &amp;quot;Corgi, Cardigan Welsh&amp;quot;         
##  [71] &amp;quot;Corgi, Pembroke Welsh&amp;quot;          &amp;quot;Coton de Tulear&amp;quot;               
##  [73] &amp;quot;Coton Retulear&amp;quot;                 &amp;quot;Curly-Coated Retriever&amp;quot;        
##  [75] &amp;quot;Curs&amp;quot;                           &amp;quot;Dachshund&amp;quot;                     
##  [77] &amp;quot;Dachshund, Long-Haired&amp;quot;         &amp;quot;Dachshund, Minature&amp;quot;           
##  [79] &amp;quot;Dachshund, Standard&amp;quot;            &amp;quot;Dachshund, Wirehaired&amp;quot;         
##  [81] &amp;quot;Dalmatian&amp;quot;                      &amp;quot;Dandie Dinmont Terrier&amp;quot;        
##  [83] &amp;quot;Dingo&amp;quot;                          &amp;quot;Doberman Pinscher&amp;quot;             
##  [85] &amp;quot;Dogo de Argentino&amp;quot;              &amp;quot;Dogue De Bordeaux&amp;quot;             
##  [87] &amp;quot;Elkhound&amp;quot;                       &amp;quot;English Foxhound&amp;quot;              
##  [89] &amp;quot;English Setter&amp;quot;                 &amp;quot;English Springer Spaniel&amp;quot;      
##  [91] &amp;quot;English Toy Spaniel&amp;quot;            &amp;quot;Entelbucher&amp;quot;                   
##  [93] &amp;quot;Field Spaniel&amp;quot;                  &amp;quot;Finnish Spitz&amp;quot;                 
##  [95] &amp;quot;Flat-Coated Retriever&amp;quot;          &amp;quot;Formosan Mountain Dog&amp;quot;         
##  [97] &amp;quot;Fox Terrier&amp;quot;                    &amp;quot;Fox Terrier, Smooth&amp;quot;           
##  [99] &amp;quot;Fox Terrier, Toy&amp;quot;               &amp;quot;Fox Terrier, Wirehaired&amp;quot;       
## [101] &amp;quot;French Bulldog&amp;quot;                 &amp;quot;German Shepherd&amp;quot;               
## [103] &amp;quot;German Shorthair Pointer&amp;quot;       &amp;quot;German Wirehair Pointer&amp;quot;       
## [105] &amp;quot;Giffon Vendeen&amp;quot;                 &amp;quot;Golden Retriever&amp;quot;              
## [107] &amp;quot;Goldendoodle&amp;quot;                   &amp;quot;Gordon Setter&amp;quot;                 
## [109] &amp;quot;Great Dane&amp;quot;                     &amp;quot;Great Pyrenees&amp;quot;                
## [111] &amp;quot;Greater Swiss Mountain Dog&amp;quot;     &amp;quot;Greyhound&amp;quot;                     
## [113] &amp;quot;Harrier&amp;quot;                        &amp;quot;Havanese&amp;quot;                      
## [115] &amp;quot;Hound&amp;quot;                          &amp;quot;Husky&amp;quot;                         
## [117] &amp;quot;Ibizan Hound&amp;quot;                   &amp;quot;Irish Setter&amp;quot;                  
## [119] &amp;quot;Irish Terrier&amp;quot;                  &amp;quot;Irish Wolfhound&amp;quot;               
## [121] &amp;quot;Italian Greyhound&amp;quot;              &amp;quot;Italian Spinone&amp;quot;               
## [123] &amp;quot;Jack Russell Terrier&amp;quot;           &amp;quot;Japanese Chin&amp;quot;                 
## [125] &amp;quot;Japanese Fox&amp;quot;                   &amp;quot;Kairn Terrier&amp;quot;                 
## [127] &amp;quot;Karelian Bear Dog&amp;quot;              &amp;quot;Keeshond&amp;quot;                      
## [129] &amp;quot;Kerry Blue Terrier&amp;quot;             &amp;quot;King Charles Spaniel&amp;quot;          
## [131] &amp;quot;Kookier Hound&amp;quot;                  &amp;quot;Korean Chin-do&amp;quot;                
## [133] &amp;quot;Kuvasz&amp;quot;                         &amp;quot;Kyileo&amp;quot;                        
## [135] &amp;quot;Lab Retriever&amp;quot;                  &amp;quot;Labradoodle&amp;quot;                   
## [137] &amp;quot;Labrador Retriever&amp;quot;             &amp;quot;Lakeland Terrier&amp;quot;              
## [139] &amp;quot;Landseer&amp;quot;                       &amp;quot;Leonberger&amp;quot;                    
## [141] &amp;quot;Lhaso Apso&amp;quot;                     &amp;quot;Looks Like&amp;quot;                    
## [143] &amp;quot;Lowchen&amp;quot;                        &amp;quot;Maltese&amp;quot;                       
## [145] &amp;quot;Manchester Terrier&amp;quot;             &amp;quot;Manchester Terrier, Toy&amp;quot;       
## [147] &amp;quot;Mastiff&amp;quot;                        &amp;quot;McNab&amp;quot;                         
## [149] &amp;quot;Mexican Hairless&amp;quot;               &amp;quot;Miniture Pinscher&amp;quot;             
## [151] &amp;quot;Mix&amp;quot;                            &amp;quot;Neapolitan Mastiff&amp;quot;            
## [153] &amp;quot;Newfoundland&amp;quot;                   &amp;quot;Norfolk Terrier&amp;quot;               
## [155] &amp;quot;Norwegian Elkhound&amp;quot;             &amp;quot;Norwegian Lundehund&amp;quot;           
## [157] &amp;quot;Norwich Terrier&amp;quot;                &amp;quot;Novia Scotia Duck Tolling Retr&amp;quot;
## [159] &amp;quot;NULL&amp;quot;                           &amp;quot;Old English Sheepdog&amp;quot;          
## [161] &amp;quot;Otterhound&amp;quot;                     &amp;quot;Papillon&amp;quot;                      
## [163] &amp;quot;Pekingese&amp;quot;                      &amp;quot;Pharaoh Hound&amp;quot;                 
## [165] &amp;quot;Pitbull&amp;quot;                        &amp;quot;Plott Hound&amp;quot;                   
## [167] &amp;quot;Pointer&amp;quot;                        &amp;quot;Pointing Griffon, Wirehaired&amp;quot;  
## [169] &amp;quot;Pomeranian&amp;quot;                     &amp;quot;Poodle&amp;quot;                        
## [171] &amp;quot;Poodle, Minature&amp;quot;               &amp;quot;Poodle, Standard&amp;quot;              
## [173] &amp;quot;Poodle, Teacup&amp;quot;                 &amp;quot;Poodle, Toy&amp;quot;                   
## [175] &amp;quot;Portuguese Water Dog&amp;quot;           &amp;quot;Pug&amp;quot;                           
## [177] &amp;quot;Puli&amp;quot;                           &amp;quot;Purebred&amp;quot;                      
## [179] &amp;quot;Queensland Blue Heeler&amp;quot;         &amp;quot;Red Heeler&amp;quot;                    
## [181] &amp;quot;Rhodesian Ridgeback&amp;quot;            &amp;quot;Rottweiler&amp;quot;                    
## [183] &amp;quot;Saint Bernard&amp;quot;                  &amp;quot;Saluki&amp;quot;                        
## [185] &amp;quot;Samoyed&amp;quot;                        &amp;quot;Schipperke&amp;quot;                    
## [187] &amp;quot;Schnauzer&amp;quot;                      &amp;quot;Schnauzer, Giant&amp;quot;              
## [189] &amp;quot;Schnauzer, Minature&amp;quot;            &amp;quot;Schnauzer, Standard&amp;quot;           
## [191] &amp;quot;Scottish Deerhound&amp;quot;             &amp;quot;Scottish Terrier&amp;quot;              
## [193] &amp;quot;Sealyham Terrier&amp;quot;               &amp;quot;See Notes&amp;quot;                     
## [195] &amp;quot;Setter&amp;quot;                         &amp;quot;Shar-Pei&amp;quot;                      
## [197] &amp;quot;Shepherd&amp;quot;                       &amp;quot;Shetland Sheepdog&amp;quot;             
## [199] &amp;quot;Shiba Inu&amp;quot;                      &amp;quot;Shih Tzu&amp;quot;                      
## [201] &amp;quot;Siberian Husky&amp;quot;                 &amp;quot;Silken Windhound&amp;quot;              
## [203] &amp;quot;Silky Terrier&amp;quot;                  &amp;quot;Spaniel&amp;quot;                       
## [205] &amp;quot;Springer Spaniel&amp;quot;               &amp;quot;Staffordshire Bull Terrier&amp;quot;    
## [207] &amp;quot;Sussex Spaniel&amp;quot;                 &amp;quot;Terrier&amp;quot;                       
## [209] &amp;quot;Terrier, Black Russian&amp;quot;         &amp;quot;Terrier, Rat&amp;quot;                  
## [211] &amp;quot;Terrier, Soft-Coated Wheaten&amp;quot;   &amp;quot;Tibetan Mastiff&amp;quot;               
## [213] &amp;quot;Tibetan Spaniel&amp;quot;                &amp;quot;Tibetan Terrier&amp;quot;               
## [215] &amp;quot;Unspecified&amp;quot;                    &amp;quot;Vizsla&amp;quot;                        
## [217] &amp;quot;Water Spaniel&amp;quot;                  &amp;quot;Weimaraner&amp;quot;                    
## [219] &amp;quot;Welsh Springer Spaniel&amp;quot;         &amp;quot;Welsh Terrier&amp;quot;                 
## [221] &amp;quot;West Highland Terrier&amp;quot;          &amp;quot;West Highland White Terrier&amp;quot;   
## [223] &amp;quot;West Hihgland White Terrier&amp;quot;    &amp;quot;Whippet&amp;quot;                       
## [225] &amp;quot;Wirehair Terrier&amp;quot;               &amp;quot;Wolf Hybrid&amp;quot;                   
## [227] &amp;quot;Yorkshire Terrier&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow! That&amp;rsquo;s quite a few! We&amp;rsquo;ll come back to cleaning up the species names in a bit.&lt;/p&gt;

&lt;p&gt;Generally, the dog&amp;rsquo;s estimated size may be more beneficial than their breed, so let&amp;rsquo;s try to pair these data up with information from other sources. The data used are available &lt;a href=&#34;http://modernpuppies.com/breedweightchart.aspx&#34;&gt;here&lt;/a&gt;. Time to import the database of dog breeds with their sizes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weight &amp;lt;- read.csv(file = &amp;quot;Breed_Wt.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what that looks like.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(weight)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:    372 obs. of  3 variables:
##  $ Breed                 : chr  &amp;quot;Airedoodle&amp;quot; &amp;quot;Alapaha Blue Blood Bulldog&amp;quot; &amp;quot;Alaskan Klee Kai&amp;quot; &amp;quot;Alsatian&amp;quot; ...
##  $ Average.Adult.Weight  : chr  &amp;quot;Male: 45-65 lbs&amp;quot; &amp;quot;Male: 50-80 lbs&amp;quot; &amp;quot;Male: 11-15 lbs&amp;quot; &amp;quot;Male: 65-85 lbs&amp;quot; ...
##  $ Average.Adult.Weight.1: chr  &amp;quot;Female: 45-65 lbs&amp;quot; &amp;quot;Female: 50-80 lbs&amp;quot; &amp;quot;Female: 11-15 lbs&amp;quot; &amp;quot;Female: 50-70 lbs&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(weight)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##                        Breed Average.Adult.Weight Average.Adult.Weight.1
## 1                 Airedoodle      Male: 45-65 lbs      Female: 45-65 lbs
## 2 Alapaha Blue Blood Bulldog      Male: 50-80 lbs      Female: 50-80 lbs
## 3           Alaskan Klee Kai      Male: 11-15 lbs      Female: 11-15 lbs
## 4                   Alsatian      Male: 65-85 lbs      Female: 50-70 lbs
## 5           American Bulldog      Male: 45-75 lbs      Female: 45-75 lbs
## 6               Aussiedoodle     Male: 35-65 lbs       Female: 25-55 lbs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cleaning-data&#34;&gt;Cleaning Data&lt;/h2&gt;

&lt;p&gt;Looks like lots of character strings. Time to split out some of these columns. We&amp;rsquo;ll use the R packages &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; for this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Delete empty rows
weight &amp;lt;- weight[!apply(weight == &amp;quot;&amp;quot;, 1, all), ]

# Splitting Average Adult Weight (Species with Weight Ranges)
weight_split &amp;lt;- weight %&amp;gt;% separate(Average.Adult.Weight, into = c(&amp;quot;Male&amp;quot;, 
    &amp;quot;Wt_Range&amp;quot;), sep = &amp;quot;: &amp;quot;) %&amp;gt;% filter(grepl(&amp;quot;-&amp;quot;, Wt_Range)) %&amp;gt;% 
    separate(Wt_Range, into = c(&amp;quot;Min&amp;quot;, &amp;quot;Max&amp;quot;, &amp;quot;lbs&amp;quot;)) %&amp;gt;% select(-lbs) %&amp;gt;% 
    separate(Average.Adult.Weight.1, into = c(&amp;quot;Female&amp;quot;, &amp;quot;Wt_Range_F&amp;quot;), 
        sep = &amp;quot;: &amp;quot;) %&amp;gt;% filter(grepl(&amp;quot;-&amp;quot;, Wt_Range_F)) %&amp;gt;% separate(Wt_Range_F, 
    into = c(&amp;quot;Min_F&amp;quot;, &amp;quot;Max_F&amp;quot;, &amp;quot;lbs_F&amp;quot;)) %&amp;gt;% mutate_each(funs(as.numeric), 
    (Min:Max)) %&amp;gt;% mutate_each(funs(as.numeric), (Min_F:Max_F)) %&amp;gt;% 
    mutate(Male_Avg = (Min + Max)/2) %&amp;gt;% mutate(Female_Avg = (Min_F + 
    Max_F)/2) %&amp;gt;% select(1, 9:10)

# Splitting Average Adult Weight (no ranges)
weight_split_2 &amp;lt;- weight %&amp;gt;% separate(Average.Adult.Weight, into = c(&amp;quot;Male&amp;quot;, 
    &amp;quot;Wt_Range&amp;quot;), sep = &amp;quot;: &amp;quot;) %&amp;gt;% filter(!grepl(&amp;quot;-&amp;quot;, Wt_Range)) %&amp;gt;% 
    separate(Wt_Range, into = c(&amp;quot;Male_Avg&amp;quot;, &amp;quot;lbs&amp;quot;)) %&amp;gt;% separate(Average.Adult.Weight.1, 
    into = c(&amp;quot;Female&amp;quot;, &amp;quot;Wt_Range_F&amp;quot;), sep = &amp;quot;: &amp;quot;) %&amp;gt;% separate(Wt_Range_F, 
    into = c(&amp;quot;Female_Avg&amp;quot;, &amp;quot;lbs_2&amp;quot;)) %&amp;gt;% select(1, 3, 6)

# Bind Both Data Frames
weight_split_all &amp;lt;- rbind(weight_split_2, weight_split)

# Check out our new dataframe
head(weight_split_all)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##              Breed Male_Avg Female_Avg
## 1     Afghan Hound       60         50
## 2 Airedale Terrier       55         55
## 3 Alaskan Malamute       85         75
## 4          Basenji       24         22
## 5  English Bulldog       50         40
## 6    Cairn Terrier       14         13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Now we have a dataframe that just lists breed, and then average weight for males and females. Now we can use this to fill in the estimated weight for each of the animals in our &lt;code&gt;dogs&lt;/code&gt; dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make a copy of our dataset
dogs_2 &amp;lt;- dogs

# Match dog breed from dogs dataset to breed from
# weight_split_all dataset
dogs_2$Male_Avg &amp;lt;- weight_split_all[match(dogs_2$Primary.Breed, 
    weight_split_all$Breed), &amp;quot;Male_Avg&amp;quot;]

dogs_2$Male_Avg &amp;lt;- as.numeric(dogs_2$Male_Avg)

dogs_2$Female_Avg &amp;lt;- weight_split_all[match(dogs_2$Primary.Breed, 
    weight_split_all$Breed), &amp;quot;Female_Avg&amp;quot;]

dogs_2$Female_Avg &amp;lt;- as.numeric(dogs_2$Female_Avg)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Let&amp;rsquo;s check how we did.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(dogs_2$Male_Avg)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##    5.00   16.00   52.50   47.53   72.50  182.50   11611
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, looks like our system automatically placed 73% of our entries, the remaining 27% (or 11,611 dogs) were not found. Let&amp;rsquo;s see if those were all the same or similar species.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;missing_size &amp;lt;- dogs_2 %&amp;gt;% filter(is.na(Male_Avg)) %&amp;gt;% group_by(Primary.Breed) %&amp;gt;% 
    summarise(count = n())

head(missing_size)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##                Primary.Breed count
##                       &amp;lt;fctr&amp;gt; &amp;lt;int&amp;gt;
## 1           Alaskan Malumute   123
## 2      Amer. Pitbull Terrier   137
## 3        Amer. Water Spaniel     5
## 4 Amer.Staffordshire Terrier   186
## 5      Appenzel Mountain Dog     3
## 6         Australian Shepard  1216
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ah, looks like we have a few misspelled names in our original database. Let&amp;rsquo;s try to clean those up a bit.&lt;/p&gt;

&lt;p&gt;Due to the random nature of the misspellings and re-wording of breeds in this dataset, I&amp;rsquo;ll use the &lt;code&gt;gsub&lt;/code&gt; function to manually recode the ones that need it. This also allows for verification that the correct new term is being used.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Fixing Misspellings
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Alaskan Malumute&amp;quot;, &amp;quot;Alaskan Malamute&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Shepard&amp;quot;, &amp;quot;Shepherd&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Belgian Malinios&amp;quot;, &amp;quot;Belgian Malinois&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Bouvier De Flanders&amp;quot;, &amp;quot;Bouvier des Flandres&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Coton Retulear&amp;quot;, &amp;quot;Coton de Tulear&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Dogo de Argentino&amp;quot;, &amp;quot;Dogo Argentino&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Entelbucher&amp;quot;, &amp;quot;Entlebucher&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;German Shorthair Pointer&amp;quot;, &amp;quot;German Shorthaired Pointer&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;German Wirehair Pointer&amp;quot;, &amp;quot;German Wirehaired Pointer&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Giffon Vendeen&amp;quot;, &amp;quot;Griffon Vendeen&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Italian Spinone&amp;quot;, &amp;quot;Spinone Italiano&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Kairn Terrier&amp;quot;, &amp;quot;Cairn Terrier&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Kookier Hound&amp;quot;, &amp;quot;Kooikerhondje&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Korean Chin-do&amp;quot;, &amp;quot;Jindo&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Kyileo&amp;quot;, &amp;quot;Kyi-Leo&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Lhaso Apso&amp;quot;, &amp;quot;Lhasa Apso&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Miniture Pinscher&amp;quot;, &amp;quot;Miniature Pinscher&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Novia Scotia Duck Tolling Retr&amp;quot;, 
    &amp;quot;Nova Scotia Duck Tolling Retriever&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Setter$&amp;quot;, &amp;quot;English Setter&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;West Hihgland White Terrier|West Highland White Terrier&amp;quot;, 
    &amp;quot;West Highland Terrier&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Amer.Staffordshire Terrier&amp;quot;, &amp;quot;American Staffordshire Terrier&amp;quot;, 
    dogs_2$Primary.Breed)

# Change any &#39;Amer.&#39; to &#39;American&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Amer\\.$&amp;quot;, &amp;quot;American&amp;quot;, dogs_2$Primary.Breed, 
    fixed = TRUE)

# Another name for Bordeaux Mastiff is &#39;French Mastiff&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Bordeaux Mastiff|Dogue De Bordeaux&amp;quot;, 
    &amp;quot;French Mastiff&amp;quot;, dogs_2$Primary.Breed)

# Change &#39;Elkhound&#39; to &#39;Norwegian Elkhound&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Elkhound$&amp;quot;, &amp;quot;Norwegian Elkhound&amp;quot;, 
    dogs_2$Primary.Breed)

# Change &#39;Husky&#39; to &#39;Siberian Husky&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Husky$&amp;quot;, &amp;quot;Siberian Husky&amp;quot;, dogs_2$Primary.Breed)

# Change King Charles Spaniel to Cavalier King Charles
# Spaniel
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;King Charles Spaniel|Cavalier King Charles Spaniel&amp;quot;, 
    &amp;quot;Cavalier King Charles Spaniel&amp;quot;, dogs_2$Primary.Breed)

# Change Lab Retriever to Labrador Retriever
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Lab Retriever&amp;quot;, &amp;quot;Labrador Retriever&amp;quot;, 
    dogs_2$Primary.Breed)

# Mexican Hairless are also called &#39;Xolo&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Mexican Hairless&amp;quot;, &amp;quot;Xolo&amp;quot;, dogs_2$Primary.Breed)

# Queensland Blue Heelers and Red Heelers are more commonly
# known as Australian Cattle Dogs
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Queensland Blue Heeler|Red Heeler&amp;quot;, 
    &amp;quot;Australian Cattle Dog&amp;quot;, dogs_2$Primary.Breed)

# Water Spaniels are the same as American Water Spaniels
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Amer\\. Water Spaniel|^Water Spaniel$&amp;quot;, 
    &amp;quot;American Water Spaniel&amp;quot;, dogs_2$Primary.Breed)

# Japanese Foxes are also called &#39;Shiba Inu&#39;
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Japanese Fox&amp;quot;, &amp;quot;Shiba Inu&amp;quot;, dogs_2$Primary.Breed)

# Changing the order of some words
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Bull Terrier, Minature&amp;quot;, &amp;quot;Miniature Bull Terrier&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Bulldog American&amp;quot;, &amp;quot;American Bulldog&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Bulldog English&amp;quot;, &amp;quot;English Bulldog&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Cocker Spaniel, American&amp;quot;, &amp;quot;American Cocker Spaniel&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Cocker Spaniel, English&amp;quot;, &amp;quot;English Cocker Spaniel&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Corgi, Cardigan Welsh&amp;quot;, &amp;quot;Cardigan Welsh Corgi&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Corgi, Pembroke Welsh&amp;quot;, &amp;quot;Pembroke Welsh Corgi&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Dachshund, Minature&amp;quot;, &amp;quot;Miniature Dachshund&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Fox Terrier, Toy&amp;quot;, &amp;quot;Toy Fox Terrier&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Manchester Terrier, Toy&amp;quot;, &amp;quot;Toy Manchester Terrier&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Pointing Griffon, Wirehaired&amp;quot;, 
    &amp;quot;Wirehaired Pointing Griffon&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Poodle, Minature&amp;quot;, &amp;quot;Miniature Poodle&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Schnauzer, Giant&amp;quot;, &amp;quot;Giant Schnauzer&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Schnauzer, Minature&amp;quot;, &amp;quot;Miniature Schnauzer&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Terrier, Black Russian&amp;quot;, &amp;quot;Black Russian Terrier&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Terrier, Rat&amp;quot;, &amp;quot;Rat Terrier&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Terrier, Soft-Coated Wheaten&amp;quot;, 
    &amp;quot;Soft Coated Wheaten Terrier&amp;quot;, dogs_2$Primary.Breed)

# Combine similar types
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Collie, Rough-Coated|Collie, Smooth-Coated&amp;quot;, 
    &amp;quot;Collie&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Chihuahua, Long-haired&amp;quot;, &amp;quot;Chihuahua&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Coonhound, Redbone|Coonhound, Walker&amp;quot;, 
    &amp;quot;Coonhound&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Dachshund, Long-Haired|Dachshund, Standard|Dachshund, Wirehaired|^Dachshund$&amp;quot;, 
    &amp;quot;Standard Dachshund&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Fox Terrier$|Fox Terrier, Smooth|Fox Terrier, Wirehaired|Wirehair Terrier&amp;quot;, 
    &amp;quot;Standard Fox Terrier&amp;quot;, dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Poodle$|Poodle, Standard&amp;quot;, &amp;quot;Standard Poodle&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Poodle, Teacup|Poodle, Toy&amp;quot;, &amp;quot;Toy Poodle&amp;quot;, 
    dogs_2$Primary.Breed)
dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;^Schnauzer$|Schnauzer, Standard&amp;quot;, 
    &amp;quot;Standard Schnauzer&amp;quot;, dogs_2$Primary.Breed)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That was a lot of very different errors in that dataset! Let&amp;rsquo;s see if that helped.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Match dog breed from dogs dataset to breed from
# weight_split_all dataset
dogs_2$Male_Avg &amp;lt;- weight_split_all[match(dogs_2$Primary.Breed, 
    weight_split_all$Breed), &amp;quot;Male_Avg&amp;quot;]

dogs_2$Male_Avg &amp;lt;- as.numeric(dogs_2$Male_Avg)

dogs_2$Female_Avg &amp;lt;- weight_split_all[match(dogs_2$Primary.Breed, 
    weight_split_all$Breed), &amp;quot;Female_Avg&amp;quot;]

dogs_2$Female_Avg &amp;lt;- as.numeric(dogs_2$Female_Avg)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see how we did.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(dogs_2$Male_Avg)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##    5.00   15.50   50.00   45.19   72.50  182.50    3503
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, that did fix some of the missing values! Still another 3,503 (or 8%) to go!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;missing_size_2 &amp;lt;- dogs_2 %&amp;gt;% filter(is.na(Male_Avg)) %&amp;gt;% group_by(Primary.Breed) %&amp;gt;% 
    summarise(count = n())

missing_size_2$Primary.Breed
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Amer. Pitbull Terrier&amp;quot;  &amp;quot;Appenzel Mountain Dog&amp;quot; 
##  [3] &amp;quot;Belgian Shepherd&amp;quot;       &amp;quot;Bulldog&amp;quot;               
##  [5] &amp;quot;Carolina&amp;quot;               &amp;quot;Cirneco Dell Etna&amp;quot;     
##  [7] &amp;quot;Curs&amp;quot;                   &amp;quot;Dingo&amp;quot;                 
##  [9] &amp;quot;Entlebucher&amp;quot;            &amp;quot;Formosan Mountain Dog&amp;quot; 
## [11] &amp;quot;Hound&amp;quot;                  &amp;quot;Kyi-Leo&amp;quot;               
## [13] &amp;quot;Landseer&amp;quot;               &amp;quot;Looks Like&amp;quot;            
## [15] &amp;quot;McNab&amp;quot;                  &amp;quot;Mix&amp;quot;                   
## [17] &amp;quot;Norwegian Lundehund&amp;quot;    &amp;quot;NULL&amp;quot;                  
## [19] &amp;quot;Purebred&amp;quot;               &amp;quot;Saluki&amp;quot;                
## [21] &amp;quot;See Notes&amp;quot;              &amp;quot;Shepherd&amp;quot;              
## [23] &amp;quot;Silken Windhound&amp;quot;       &amp;quot;Spaniel&amp;quot;               
## [25] &amp;quot;Terrier&amp;quot;                &amp;quot;Toy Manchester Terrier&amp;quot;
## [27] &amp;quot;Unspecified&amp;quot;            &amp;quot;Welsh Springer Spaniel&amp;quot;
## [29] &amp;quot;Wolf Hybrid&amp;quot;            &amp;quot;Xolo&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like a few of these can&amp;rsquo;t be sorted. For example, we have no way of estimating size for &amp;ldquo;Looks Like&amp;rdquo;, &amp;ldquo;Mix&amp;rdquo;, &amp;ldquo;Purebred&amp;rdquo;, &amp;ldquo;See Notes&amp;rdquo; or &amp;ldquo;Unspecified&amp;rdquo;, so I&amp;rsquo;ll make the sizes of all of those &amp;ldquo;NA&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs_2$Primary.Breed &amp;lt;- gsub(&amp;quot;Looks Like|^Mix$|NULL|Purebred|See Notes|Unspecified|^Curs$&amp;quot;, 
    NA, dogs_2$Primary.Breed)

# Calculate how many are missing now
missing_size_3 &amp;lt;- dogs_2 %&amp;gt;% filter(is.na(Male_Avg)) %&amp;gt;% group_by(Primary.Breed) %&amp;gt;% 
    summarise(count = n())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So of our remaining 3,503 dogs, how many of them have sizes that we can&amp;rsquo;t estimate?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;missing_size_3[which(is.na(missing_size_3)), ]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   Primary.Breed count
##           &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
## 1          &amp;lt;NA&amp;gt;  1313
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can&amp;rsquo;t estimate 1,313 dogs&amp;rsquo; sizes, which means that we can still estimate the size of the remaining 2,190 dogs. We only have a few species left, and it looks like they weren&amp;rsquo;t ones that were in our original weights dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;missing_size_3$Primary.Breed
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Amer. Pitbull Terrier&amp;quot;  &amp;quot;Appenzel Mountain Dog&amp;quot; 
##  [3] &amp;quot;Belgian Shepherd&amp;quot;       &amp;quot;Bulldog&amp;quot;               
##  [5] &amp;quot;Carolina&amp;quot;               &amp;quot;Cirneco Dell Etna&amp;quot;     
##  [7] &amp;quot;Dingo&amp;quot;                  &amp;quot;Entlebucher&amp;quot;           
##  [9] &amp;quot;Formosan Mountain Dog&amp;quot;  &amp;quot;Hound&amp;quot;                 
## [11] &amp;quot;Kyi-Leo&amp;quot;                &amp;quot;Landseer&amp;quot;              
## [13] &amp;quot;McNab&amp;quot;                  &amp;quot;Norwegian Lundehund&amp;quot;   
## [15] &amp;quot;Saluki&amp;quot;                 &amp;quot;Shepherd&amp;quot;              
## [17] &amp;quot;Silken Windhound&amp;quot;       &amp;quot;Spaniel&amp;quot;               
## [19] &amp;quot;Terrier&amp;quot;                &amp;quot;Toy Manchester Terrier&amp;quot;
## [21] &amp;quot;Welsh Springer Spaniel&amp;quot; &amp;quot;Wolf Hybrid&amp;quot;           
## [23] &amp;quot;Xolo&amp;quot;                   NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since there are only a few breeds left, I will manually create a database of the average weights (all found on the Wikipedia pages for the species).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll upload in the supplemental file and match the breed name and mass.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Import File
supp_weight &amp;lt;- read.csv(file = &amp;quot;Supp_Breed_Wt.csv&amp;quot;, header = TRUE, 
    stringsAsFactors = FALSE)

# Make a copy of our filtered dataset
dogs_3 &amp;lt;- dogs_2 %&amp;gt;% filter(is.na(Male_Avg))

dogs_4 &amp;lt;- dogs_2 %&amp;gt;% filter(!is.na(Male_Avg))

# Match dog breed from dogs dataset to breed from
# weight_split_all dataset
dogs_3$Male_Avg &amp;lt;- supp_weight[match(dogs_3$Primary.Breed, supp_weight$Breed), 
    &amp;quot;Male_Avg&amp;quot;]

dogs_3$Male_Avg &amp;lt;- as.numeric(dogs_3$Male_Avg)

dogs_3$Female_Avg &amp;lt;- supp_weight[match(dogs_3$Primary.Breed, 
    supp_weight$Breed), &amp;quot;Female_Avg&amp;quot;]

dogs_3$Female_Avg &amp;lt;- as.numeric(dogs_3$Female_Avg)

# Bind datasets back together
dogs_5 &amp;lt;- rbind(dogs_3, dogs_4)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(dogs_5$Male_Avg)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##    5.00   16.00   50.00   45.77   72.50  182.50    1313
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Awesome! So the only dogs that we still don&amp;rsquo;t have approximate weights on are the ones with miscoded breeds. Now let&amp;rsquo;s get a better approximation of weight based on gender.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs_5$Gender &amp;lt;- as.factor(dogs_5$Gender)

dogs_5 &amp;lt;- dogs_5 %&amp;gt;% mutate(weight = ifelse(Gender == &amp;quot;Female&amp;quot;, 
    Female_Avg, Male_Avg))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perfect. Now we can make an estimate of how they would be categorized on &amp;ldquo;&lt;a href=&#34;www.rover.com&#34;&gt;Rover.com&lt;/a&gt;&amp;rdquo;, a Seattle-based website aimed at helping dog-owners find reliable dog-sitters and walkers. Their website lists the following size cutoffs (in lbs):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Small = 0 - 15&lt;/li&gt;
&lt;li&gt;Medium = 16 - 40&lt;/li&gt;
&lt;li&gt;Large = 41 - 100&lt;/li&gt;
&lt;li&gt;Giant = 100 +&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s add those classifications to our dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs_5 &amp;lt;- dogs_5 %&amp;gt;% mutate(size_class = ifelse(weight &amp;lt;= 15, 
    &amp;quot;Small&amp;quot;, ifelse(weight &amp;gt; 15 &amp;amp; weight &amp;lt;= 40, &amp;quot;Medium&amp;quot;, ifelse(weight &amp;gt; 
        40 &amp;amp; weight &amp;lt;= 100, &amp;quot;Large&amp;quot;, ifelse(weight &amp;gt; 100, &amp;quot;Giant&amp;quot;, 
        NA)))))

dogs_5$size_class &amp;lt;- as.factor(dogs_5$size_class)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ve got information on dogs, with their breeds, zipcodes and approximate sizes and size classes. This will help us later.&lt;/p&gt;

&lt;h2 id=&#34;data-visualizations&#34;&gt;Data Visualizations&lt;/h2&gt;

&lt;h3 id=&#34;dog-popularity-by-breed&#34;&gt;Dog Popularity &amp;ndash; By Breed&lt;/h3&gt;

&lt;p&gt;Now let&amp;rsquo;s look at some visualizations. We&amp;rsquo;ll start with the popularity of each dog breed within Seattle. This figure shows the top 24 most popular breeds in the city.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-23-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Looks like labrador retrievers are by far the most popular breed of dog in Seattle.&lt;/p&gt;

&lt;h3 id=&#34;dog-popularity-by-size&#34;&gt;Dog Popularity &amp;ndash; By Size&lt;/h3&gt;

&lt;p&gt;How do the numbers of dogs break down by dog size?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-24-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For a city filled with apartment buildings, Seattle-ites really love large dogs (almost as much as medium and small dogs combined!).&lt;/p&gt;

&lt;p&gt;I wonder if this trend varies depending on which part of the city the pets are living in.&lt;/p&gt;

&lt;h3 id=&#34;dog-populations-by-zip-code&#34;&gt;Dog Populations by Zip Code&lt;/h3&gt;

&lt;p&gt;First let&amp;rsquo;s see the density of dogs living in each zip code. Looks like we have some messy zip code data with quite a few zip codes not falling in the Seattle area. We&amp;rsquo;ll use a list of &lt;a href=&#34;http://www.unitedstateszipcodes.org/wa/#zips-list&#34;&gt;Washington State zip codes&lt;/a&gt; to narrow this list down.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Import Zipcode dataset
zipcodes &amp;lt;- read.csv(&amp;quot;Zipcodes.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)

# Match zipcodes in database to those listed on dog licenses
dogs_5$zip &amp;lt;- zipcodes[match(dogs_5$Zip.C, zipcodes$ZIP), &amp;quot;ZIP&amp;quot;]

# Make zipcodes factors
dogs_5$zip &amp;lt;- as.factor(dogs_5$zip)

# How many are missing?
sum(is.na(dogs_5$zip))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 365
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so there were 365 dog licenses that didn&amp;rsquo;t list appropriate zip codes. That&amp;rsquo;s alright, that&amp;rsquo;s less than 1% of our dataset. Let&amp;rsquo;s move forward.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It looks like several of the zipcodes listed are &amp;ldquo;industrial&amp;rdquo; zip codes. We&amp;rsquo;ll replace the zip code with the residential zip code that each industrial one falls within.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-26-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like the highest populations of licensed dogs are found in zipcodes 98115, 98103, and 98117. That corresponds roughly to Northeast Seattle, the area between Fremont and Greenwood, and the Ballard to Crown Hill area. It is certainly possible that the high dog populations in those areas could be correlated with the human population in the same areas. Let&amp;rsquo;s see how many licensed dogs there are per person in these areas.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Human Population Data Obtained &lt;a href=&#34;http://zipatlas.com/us/wa/seattle/zip-code-comparison/population-density.htm&#34;&gt;Here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;human_pop &amp;lt;- read.csv(&amp;quot;Human_Pop.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)

# Make a copy of our dataset (dog populations by zipcode)
pop_zip_2 &amp;lt;- pop_zip

# Match human populations from new dataset to zipcodes from
# dog population dataset
pop_zip_2$h_pop &amp;lt;- human_pop[match(pop_zip_2$region, human_pop$Zip), 
    &amp;quot;Population&amp;quot;]
pop_zip_2$h_pop &amp;lt;- gsub(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;, pop_zip_2$h_pop)
pop_zip_2$h_pop &amp;lt;- as.numeric(pop_zip_2$h_pop)


# Create new factor: dog_human (i.e. ratio of dogs to humans)
pop_zip_2 &amp;lt;- pop_zip_2 %&amp;gt;% filter(!is.na(h_pop)) %&amp;gt;% filter(h_pop &amp;gt; 
    10) %&amp;gt;% mutate(dog_human = value/h_pop) %&amp;gt;% select(1, 4)

# Change variable names for choroplethrZip
pop_zip_2 &amp;lt;- rename(pop_zip_2, value = dog_human)

# Plot
zip_choropleth(pop_zip_2, zip_zoom = (pop_zip_2$region), legend = &amp;quot;Dog:Human Ratio&amp;quot;, 
    reference_map = TRUE, num_colors = 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-27-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Generally speaking, central Seattle has a higher &amp;ldquo;Licensed Dogs : People&amp;rdquo; Ratio than the neighborhoods along the north and south edges. The highest proportion is found in zipcode 98117 or the Ballard to Crown Hill area with roughly 1 licensed dog for every 10 people. That is a residential area with lots of homes and fewer apartment buildings than the downtown-area.&lt;/p&gt;

&lt;p&gt;I wonder if there are more large dogs in those house-filled areas and small dogs in apartment-laden areas. Let&amp;rsquo;s map dog populations by their size.&lt;/p&gt;

&lt;p&gt;These figures will be proportions of small, medium, large and giant dogs in proportion to the number of dogs in each zipcode.&lt;/p&gt;

&lt;h3 id=&#34;dog-populations-by-size&#34;&gt;Dog Populations by Size&lt;/h3&gt;

&lt;h4 id=&#34;small&#34;&gt;Small&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-28-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;medium&#34;&gt;Medium&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-29-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;large&#34;&gt;Large&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-30-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;giant&#34;&gt;Giant&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-31-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;caveats-of-dog-size-by-zipcode&#34;&gt;Caveats of Dog Size by Zipcode&lt;/h3&gt;

&lt;p&gt;Wow! Looks like the largest proportion of small dogs is quite concentrated to zipcodes 98134 (Industrial District) and 98101 (Downtown Seattle, near Pike Place Market). These zip codes do have relatively small total dog populations though, totalling at only 38 and 487 dogs, respectively.&lt;/p&gt;

&lt;p&gt;Medium, large and giant sized dogs have a generally more consistent proportional distribution throughout the city. Both medium and large dogs have a higher than average proportion in zip code 98155, but again, this area boasts a small overall dog population (17 dogs total).&lt;/p&gt;

&lt;h3 id=&#34;dog-names&#34;&gt;Dog Names&lt;/h3&gt;

&lt;p&gt;Just for fun, let&amp;rsquo;s take a look at the most popular dog names in Seattle. Word cloud, anyone?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a corpus
names &amp;lt;- Corpus(VectorSource(dogs_5$Name))

# Convert to plain text document
names &amp;lt;- tm_map(names, PlainTextDocument)

# Remove numbers and punctuation, just in case
names &amp;lt;- tm_map(names, removeNumbers)
names &amp;lt;- tm_map(names, removePunctuation)

# Make all names lowercase
names &amp;lt;- tm_map(names, content_transformer(tolower))


# Generate the wordcloud
wordcloud(names, scale = c(5, 0.2), max.words = 150, random.order = FALSE, 
    rot.per = 0.35, use.r.layout = TRUE, colors = brewer.pal(6, 
        &amp;quot;Greens&amp;quot;)[c(4, 5, 6, 7, 8, 9)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-32-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Oops! Looks like a few of those are probably not real names. We&amp;rsquo;ll go ahead and remove &amp;ldquo;dog&amp;rdquo;, &amp;ldquo;null&amp;rdquo;, &amp;ldquo;altered&amp;rdquo;, &amp;ldquo;female&amp;rdquo;, &amp;ldquo;male&amp;rdquo;, &amp;ldquo;labrador&amp;rdquo;, &amp;ldquo;retriever&amp;rdquo;, &amp;ldquo;year&amp;rdquo;, and &amp;ldquo;seattle&amp;rdquo; from the wordcloud.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Remove non-names
names_2 &amp;lt;- tm_map(names, removeWords, c(&amp;quot;dog&amp;quot;, &amp;quot;null&amp;quot;, &amp;quot;seattle&amp;quot;, 
    &amp;quot;altered&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;labrador&amp;quot;, &amp;quot;retriever&amp;quot;, &amp;quot;year&amp;quot;))

# Generate the wordcloud
wordcloud(names_2, scale = c(5, 0.2), max.words = 150, random.order = FALSE, 
    rot.per = 0.25, use.r.layout = TRUE, colors = brewer.pal(6, 
        &amp;quot;Greens&amp;quot;)[c(4, 5, 6, 7, 8, 9)])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Seattle_Dogs_files/figure-markdown_github/unnamed-chunk-33-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Lucy looks like the clear winner here!&lt;/p&gt;

&lt;p&gt;Looks like some other well-known dog names (like Buddy) are pretty common in Seattle. How about &amp;ldquo;Rover&amp;rdquo;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dogs_5 %&amp;gt;% filter(Name == &amp;quot;Rover&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##            License.Type.Sold Gender          Primary.Breed Primary.Color
## 1         Dog Altered 1 year   Male        German Shepherd          Rust
## 2         Dog Altered 1 year   Male     Labrador Retriever       Chocola
## 3         Dog Altered 2 year   Male                Maltese           Red
## 4         Dog Altered 2 year   Male     Labrador Retriever         Black
## 5         Dog Altered 2 year   Male  Australian Cattle Dog         Black
## 6 Dog Unaltered SR/HC 2 year   Male American Water Spaniel         Liver
##    Name Zip.C Male_Avg Female_Avg weight size_class   zip
## 1 Rover 98112     85.0       85.0   85.0      Large 98112
## 2 Rover 98103     72.5       62.5   72.5      Large 98103
## 3 Rover 98103      5.5        5.5    5.5      Small 98103
## 4 Rover 98122     72.5       62.5   72.5      Large 98122
## 5 Rover 98125     40.0       40.0   40.0     Medium 98125
## 6 Rover 98117     37.5       32.5   37.5     Medium 98117
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are 6 licensed dogs in Seattle named Rover!&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Dogs are basically everywhere in Seattle, but are most highly concentrated closer to the center of the city rather than on bordering neighborhoods.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Regardless of apartment-living, Seattle-ites are big fans of big dogs throughout the city.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The highest dog to human ratio is found in the Ballard to Crown Hill area, with nearly 1 dog per 10 people.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generally, Seattle would be a great place to be a dog sitter or walker. To increase the likelihood of finding customers, I&amp;rsquo;d suggest being open to walking or pet-sitting large dogs where possible.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For a company like Rover.com which aims to connect dog-parents to dog sitters and walkers, I&amp;rsquo;d recommend providing this type of city-wide breakdown to potential sitters. I&amp;rsquo;d also suggest reaching out to large-dog owners to investigate their interest in becoming a sitter of other large dogs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Without both sitter and user data from Rover.com, I am unable to make recommendations regarding the best neighborhood to become a dog sitter.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pronto! Bicycle Sharing in Seattle</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Bicycle_Sharing_2/</link>
      <pubDate>Thu, 10 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Bicycle_Sharing_2/</guid>
      <description>&lt;p&gt;Data exploration, mapping, and data viz in RMarkdown.
&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is an exploration of bicycle-sharing data in the city of Seattle, WA (USA) from October 2014 - August 2016. I hope to eventually combine this data with other forms of ride-sharing and transportation in the city, but this will be the first step.&lt;/p&gt;

&lt;p&gt;Time to get started!&lt;/p&gt;

&lt;h3 id=&#34;loading-necessary-packages&#34;&gt;Loading Necessary Packages&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# For data manipulation and tidying
library(dplyr)
library(lubridate)
library(tidyr)

# For mapping
library(ggmap)
library(mapproj)

# For data visualizations
library(ggplot2)

# For modeling and machine learning
library(caret)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h3&gt;

&lt;p&gt;All of the data can be downloaded from the bicycle-sharing service &lt;a href=&#34;https://www.prontocycleshare.com/data&#34;&gt;&amp;ldquo;Pronto!&amp;rdquo;&amp;rsquo;s website&lt;/a&gt; or from &lt;a href=&#34;https://www.kaggle.com/pronto/cycle-share-dataset&#34;&gt;Kaggle&lt;/a&gt;. This project contains 3 data sets and I&amp;rsquo;ll import and inspect each data file independently.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;station &amp;lt;- read.csv(file = &amp;quot;2016_station_data.csv&amp;quot;, header = TRUE, 
    stringsAsFactors = FALSE)

trip &amp;lt;- read.csv(file = &amp;quot;2016_trip_data.csv&amp;quot;, header = TRUE, 
    stringsAsFactors = FALSE)

weather &amp;lt;- read.csv(file = &amp;quot;2016_weather_data.csv&amp;quot;, header = TRUE, 
    stringsAsFactors = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, let&amp;rsquo;s take a look at each of these data files.&lt;/p&gt;

&lt;h4 id=&#34;data-structures-and-variables&#34;&gt;Data Structures and Variables&lt;/h4&gt;

&lt;h5 id=&#34;station&#34;&gt;Station&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 58
## Variables: 9
## $ station_id        &amp;lt;chr&amp;gt; &amp;quot;BT-01&amp;quot;, &amp;quot;BT-03&amp;quot;, &amp;quot;BT-04&amp;quot;, &amp;quot;BT-05&amp;quot;, &amp;quot;CBD-03&amp;quot;...
## $ name              &amp;lt;chr&amp;gt; &amp;quot;3rd Ave &amp;amp; Broad St&amp;quot;, &amp;quot;2nd Ave &amp;amp; Vine St&amp;quot;, &amp;quot;...
## $ lat               &amp;lt;dbl&amp;gt; 47.61842, 47.61583, 47.61609, 47.61311, 47.6...
## $ long              &amp;lt;dbl&amp;gt; -122.3510, -122.3486, -122.3411, -122.3442, ...
## $ install_date      &amp;lt;chr&amp;gt; &amp;quot;10/13/2014&amp;quot;, &amp;quot;10/13/2014&amp;quot;, &amp;quot;10/13/2014&amp;quot;, &amp;quot;1...
## $ install_dockcount &amp;lt;int&amp;gt; 18, 16, 16, 14, 20, 18, 20, 20, 20, 18, 16, ...
## $ modification_date &amp;lt;chr&amp;gt; &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;11/9/2015&amp;quot;, &amp;quot;&amp;quot;,...
## $ current_dockcount &amp;lt;int&amp;gt; 18, 16, 16, 14, 20, 18, 20, 18, 20, 18, 0, 1...
## $ decommission_date &amp;lt;chr&amp;gt; &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;8/9...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like this dataset is dealing with 9 variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Station ID&lt;/strong&gt; : The individual ID number for a bike station&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; : The name of that station ID, also appears to be the rough location of the station&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latitude&lt;/strong&gt; : The latitude of the station&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Longitude&lt;/strong&gt; : The longitude of the station&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install Date&lt;/strong&gt; : When that particular station was installed (in MM/DD/YYYY format)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Install Dock Count&lt;/strong&gt; : Number of docks (bike positions) available at each station on installation day&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modification Date&lt;/strong&gt; : When a particular station was modified (in MM/DD/YYYY format)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Current Dock Count&lt;/strong&gt; : Number of docks (bike positions) available at each station on August 31, 2016&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decommission Date&lt;/strong&gt; : The date that a particular station was put out of service (in MM/DD/YYYY format)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;trip&#34;&gt;Trip&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 236,065
## Variables: 12
## $ trip_id           &amp;lt;int&amp;gt; 431, 432, 433, 434, 435, 436, 437, 438, 439,...
## $ starttime         &amp;lt;chr&amp;gt; &amp;quot;10/13/2014 10:31&amp;quot;, &amp;quot;10/13/2014 10:32&amp;quot;, &amp;quot;10/...
## $ stoptime          &amp;lt;chr&amp;gt; &amp;quot;10/13/2014 10:48&amp;quot;, &amp;quot;10/13/2014 10:48&amp;quot;, &amp;quot;10/...
## $ bikeid            &amp;lt;chr&amp;gt; &amp;quot;SEA00298&amp;quot;, &amp;quot;SEA00195&amp;quot;, &amp;quot;SEA00486&amp;quot;, &amp;quot;SEA0033...
## $ tripduration      &amp;lt;dbl&amp;gt; 985.935, 926.375, 883.831, 865.937, 923.923,...
## $ from_station_name &amp;lt;chr&amp;gt; &amp;quot;2nd Ave &amp;amp; Spring St&amp;quot;, &amp;quot;2nd Ave &amp;amp; Spring St&amp;quot;...
## $ to_station_name   &amp;lt;chr&amp;gt; &amp;quot;Occidental Park / Occidental Ave S &amp;amp; S Wash...
## $ from_station_id   &amp;lt;chr&amp;gt; &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD...
## $ to_station_id     &amp;lt;chr&amp;gt; &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;,...
## $ usertype          &amp;lt;chr&amp;gt; &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Mem...
## $ gender            &amp;lt;chr&amp;gt; &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;, ...
## $ birthyear         &amp;lt;int&amp;gt; 1960, 1970, 1988, 1977, 1971, 1974, 1978, 19...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This dataset appears to contain 12 variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trip ID&lt;/strong&gt; : An identification number assigned to each trip (from one bike station to another)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Start Time&lt;/strong&gt; : The time and date that a bike was borrowed from a station (in MM/DD/YYYY HH:MM format)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stop Time&lt;/strong&gt; : The time and date that a bike was returned to a station (in MM/DD/YYYY HH:MM format)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bike ID&lt;/strong&gt; : The identification number for a specific bike&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trip Duration&lt;/strong&gt; : Time of trip (measured in seconds)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;From Station Name&lt;/strong&gt; : The name of the station where the bike was borrowed from&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To Station Name&lt;/strong&gt; : The name of the station where the bike was returned to&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;From Station ID&lt;/strong&gt; : The ID number of the station where the bike was borrowed from&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;To Station ID&lt;/strong&gt; : The ID number of the station where the bike was returned to&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Type&lt;/strong&gt; : Indicates whether the user was a &amp;ldquo;Member&amp;rdquo; (i.e., someone with a monthly or annual membership to Pronto!) or a &amp;ldquo;Short-Term Pass Holder&amp;rdquo; (i.e., someone who purchased a 24 hour or 3 day pass)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gender&lt;/strong&gt; : The gender of the rider (if known)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Birth Year&lt;/strong&gt; : The year that the rider was born&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;weather&#34;&gt;Weather&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 689
## Variables: 21
## $ Date                       &amp;lt;chr&amp;gt; &amp;quot;10/13/2014&amp;quot;, &amp;quot;10/14/2014&amp;quot;, &amp;quot;10/15/...
## $ Max_Temperature_F          &amp;lt;int&amp;gt; 71, 63, 62, 71, 64, 68, 73, 66, 64,...
## $ Mean_Temperature_F         &amp;lt;int&amp;gt; 62, 59, 58, 61, 60, 64, 64, 60, 58,...
## $ Min_TemperatureF           &amp;lt;int&amp;gt; 54, 55, 54, 52, 57, 59, 55, 55, 55,...
## $ Max_Dew_Point_F            &amp;lt;int&amp;gt; 55, 52, 53, 49, 55, 59, 57, 57, 52,...
## $ MeanDew_Point_F            &amp;lt;int&amp;gt; 51, 51, 50, 46, 51, 57, 55, 54, 49,...
## $ Min_Dewpoint_F             &amp;lt;int&amp;gt; 46, 50, 46, 42, 41, 55, 53, 50, 46,...
## $ Max_Humidity               &amp;lt;int&amp;gt; 87, 88, 87, 83, 87, 90, 94, 90, 87,...
## $ Mean_Humidity              &amp;lt;int&amp;gt; 68, 78, 77, 61, 72, 83, 74, 78, 70,...
## $ Min_Humidity               &amp;lt;int&amp;gt; 46, 63, 67, 36, 46, 68, 52, 67, 58,...
## $ Max_Sea_Level_Pressure_In  &amp;lt;dbl&amp;gt; 30.03, 29.84, 29.98, 30.03, 29.83, ...
## $ Mean_Sea_Level_Pressure_In &amp;lt;dbl&amp;gt; 29.79, 29.75, 29.71, 29.95, 29.78, ...
## $ Min_Sea_Level_Pressure_In  &amp;lt;dbl&amp;gt; 29.65, 29.54, 29.51, 29.81, 29.73, ...
## $ Max_Visibility_Miles       &amp;lt;int&amp;gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,...
## $ Mean_Visibility_Miles      &amp;lt;int&amp;gt; 10, 9, 9, 10, 10, 8, 10, 10, 10, 6,...
## $ Min_Visibility_Miles       &amp;lt;int&amp;gt; 4, 3, 3, 10, 6, 2, 6, 5, 6, 2, 10, ...
## $ Max_Wind_Speed_MPH         &amp;lt;int&amp;gt; 13, 10, 18, 9, 8, 10, 10, 12, 15, 1...
## $ Mean_Wind_Speed_MPH        &amp;lt;int&amp;gt; 4, 5, 7, 4, 3, 4, 3, 5, 8, 8, 9, 4,...
## $ Max_Gust_Speed_MPH         &amp;lt;chr&amp;gt; &amp;quot;21&amp;quot;, &amp;quot;17&amp;quot;, &amp;quot;25&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;1...
## $ Precipitation_In           &amp;lt;dbl&amp;gt; 0.00, 0.11, 0.45, 0.00, 0.14, 0.31,...
## $ Events                     &amp;lt;chr&amp;gt; &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Ra...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This dataset represents quite a bit of weather data in 21 variables.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt; : The date in MM/DD/YYYY format&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Temperature F&lt;/strong&gt; : The maximum temperature that day (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Temperature F&lt;/strong&gt; : The average temperature that day (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Temperature F&lt;/strong&gt; : The minimum temperature that day (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Dew Point F&lt;/strong&gt; : The maximum dew point (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Dew Point F&lt;/strong&gt; : The average dew point (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Dew Point F&lt;/strong&gt; : The minimum dew point (in degrees F)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Humidity&lt;/strong&gt; : The maximum humidity (in %)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Humidity&lt;/strong&gt; : The average humidity (in %)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Humidity&lt;/strong&gt; : The minimum humidity (in %)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maximum Sea Level Pressure&lt;/strong&gt; : The maximum atmospheric pressure at sea level (in inches of mercury)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Sea Level Pressure&lt;/strong&gt; : The average atmospheric pressure at sea level (in inches of mercury)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Sea Level Pressure&lt;/strong&gt; : The minimum atmospheric pressure at sea level (in inches of mercury)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Visibility Miles&lt;/strong&gt; : The maximum visibility (in miles)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Visibility Miles&lt;/strong&gt; : The average visibility (in miles)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Visibility Miles&lt;/strong&gt; : The minimum visibility (in miles)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Wind Speed MPH&lt;/strong&gt; : The maximum sustained wind speed (in miles per hour)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean Wind Speed MPH&lt;/strong&gt; : The average sustained wind speed (in miles per hour)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Gust Speed MPH&lt;/strong&gt; : The maximum gust wind speed (in miles per hour)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precipitation&lt;/strong&gt; : The amount of precipitation (measured in inches)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Events&lt;/strong&gt; : Weather events that occurred that day (e.g., rain, fog, snow, thunderstorm etc.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;data-visualizations&#34;&gt;Data Visualizations&lt;/h2&gt;

&lt;h3 id=&#34;exploring-the-stations-dataset&#34;&gt;Exploring the Stations Dataset&lt;/h3&gt;

&lt;p&gt;Since the &amp;ldquo;Stations&amp;rdquo; dataset was the first one I imported, let&amp;rsquo;s start with a little exploration there. First of all, how many unique stations are we dealing with?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;station %&amp;gt;% summarise(n_distinct(station_id))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   n_distinct(station_id)
## 1                     58
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow! 58 different stations! Let&amp;rsquo;s take a quick peek at where they are located.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;station_locs &amp;lt;- station %&amp;gt;% group_by(station_id) %&amp;gt;% select(1:4, 
    -2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Load the correct map
mymap &amp;lt;- get_map(location = &amp;quot;Seattle&amp;quot;, maptype = &amp;quot;roadmap&amp;quot;, zoom = 12)

# Plot a single point for each Station ID
ggmap(mymap) + geom_point(aes(x = long, y = lat), data = station_locs, 
    alpha = 0.7, color = &amp;quot;darkred&amp;quot;, size = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-8-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it looks like all of the stations are located near the Lower Queen Anne, Belltown, International District, Capitol Hill and University of Washington areas. Let&amp;rsquo;s take a more zoomed-in look.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-9-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Great! So the locations are pretty well clustered. I wonder what order they were added in.&lt;/p&gt;

&lt;h4 id=&#34;station-installations&#34;&gt;Station Installations&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s convert those character-string date objects to actual dates using the &lt;code&gt;lubridate&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;station$install_date &amp;lt;- mdy(station$install_date)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# How many times were new stations installed?
station %&amp;gt;% summarise(n_distinct(install_date))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   n_distinct(install_date)
## 1                        9
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# How many stations were installed on each date?
station %&amp;gt;% group_by(install_date) %&amp;gt;% summarise(count = n()) %&amp;gt;% 
    arrange(install_date)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 2
##   install_date count
##         &amp;lt;date&amp;gt; &amp;lt;int&amp;gt;
## 1   2014-10-13    50
## 2   2015-05-22     1
## 3   2015-06-12     1
## 4   2015-07-27     1
## 5   2015-09-15     1
## 6   2015-10-29     1
## 7   2016-03-18     1
## 8   2016-07-03     1
## 9   2016-08-09     1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like the vast majority (86%) of the stations were added on opening day. Let&amp;rsquo;s see where those original ones were and where the rest were added.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-12-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So they added more stations throughout the district that they serve, instead of adding several new stations to a single neighborhood all at once. Good to know.&lt;/p&gt;

&lt;p&gt;Now, I wonder how many bikes can be parked at each station (as of August 31,2016)?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-13-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Well that&amp;rsquo;s weird, some of the stations have a dock count of 0. I&amp;rsquo;m assuming they didn&amp;rsquo;t start that way. Let&amp;rsquo;s calculate the change in dock count from station installation to August 31, 2016 and plot it on a map.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dock_change &amp;lt;- station %&amp;gt;% group_by(station_id) %&amp;gt;% select(station_id, 
    long, lat, ends_with(&amp;quot;dockcount&amp;quot;)) %&amp;gt;% mutate(dock_change = current_dockcount - 
    install_dockcount)
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;change-in-number-of-bike-docks-per-station&#34;&gt;Change in Number of Bike Docks Per Station&lt;/h5&gt;

&lt;p&gt;Any stations with no change in number of docks are not shown here. &lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-15-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Looks like quite a few stations took away bike docks and none gained any. Perhaps those stations weren&amp;rsquo;t being used very frequently. We&amp;rsquo;ll have to look at that a bit later.&lt;/p&gt;

&lt;h4 id=&#34;current-station-size&#34;&gt;Current Station Size&lt;/h4&gt;

&lt;p&gt;I&amp;rsquo;m going to take one quick look at the current size of each station before moving on to the next dataset. &lt;em&gt;Note: I did not include any stations that were closed as of August 31, 2016 in this map&lt;/em&gt; &lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-16-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it looks like the biggest stations tend to be on the outskirts of the rest. Where there are several stations in close proximity, there tend to be fewer bike docks at each station. That makes sense, logically speaking. If you go to a station and there is no bike to rent, you can easily go to another nearby, assuming there is another nearby. In areas where the stations are more secluded, it&amp;rsquo;s more important that there be bikes and open spaces readily available for users.&lt;/p&gt;

&lt;p&gt;Alright, I&amp;rsquo;m feeling good about exploring this dataset. Time to check out the trip dataset!&lt;/p&gt;

&lt;h3 id=&#34;exploring-the-trips-dataset&#34;&gt;Exploring the Trips Dataset&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s been a while since we&amp;rsquo;ve looked at the trip dataset, so let&amp;rsquo;s take another peek at it here.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 236,065
## Variables: 12
## $ trip_id           &amp;lt;int&amp;gt; 431, 432, 433, 434, 435, 436, 437, 438, 439,...
## $ starttime         &amp;lt;chr&amp;gt; &amp;quot;10/13/2014 10:31&amp;quot;, &amp;quot;10/13/2014 10:32&amp;quot;, &amp;quot;10/...
## $ stoptime          &amp;lt;chr&amp;gt; &amp;quot;10/13/2014 10:48&amp;quot;, &amp;quot;10/13/2014 10:48&amp;quot;, &amp;quot;10/...
## $ bikeid            &amp;lt;chr&amp;gt; &amp;quot;SEA00298&amp;quot;, &amp;quot;SEA00195&amp;quot;, &amp;quot;SEA00486&amp;quot;, &amp;quot;SEA0033...
## $ tripduration      &amp;lt;dbl&amp;gt; 985.935, 926.375, 883.831, 865.937, 923.923,...
## $ from_station_name &amp;lt;chr&amp;gt; &amp;quot;2nd Ave &amp;amp; Spring St&amp;quot;, &amp;quot;2nd Ave &amp;amp; Spring St&amp;quot;...
## $ to_station_name   &amp;lt;chr&amp;gt; &amp;quot;Occidental Park / Occidental Ave S &amp;amp; S Wash...
## $ from_station_id   &amp;lt;chr&amp;gt; &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD-06&amp;quot;, &amp;quot;CBD...
## $ to_station_id     &amp;lt;chr&amp;gt; &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;, &amp;quot;PS-04&amp;quot;,...
## $ usertype          &amp;lt;chr&amp;gt; &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Member&amp;quot;, &amp;quot;Mem...
## $ gender            &amp;lt;chr&amp;gt; &amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Male&amp;quot;, ...
## $ birthyear         &amp;lt;int&amp;gt; 1960, 1970, 1988, 1977, 1971, 1974, 1978, 19...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great, so there are quite a few things that we can potentially look at using this dataset by itself. Let&amp;rsquo;s start with the number of trips per day since Pronto! began opening bike stations. To do that, we need to recode our start date/times as POSIXct objects. We&amp;rsquo;ll use the &lt;code&gt;lubridate&lt;/code&gt; package for this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make the start and stop dates into POSIXct objects
trip_2 &amp;lt;- trip %&amp;gt;% mutate(start_dt = mdy_hm(starttime), stop_dt = mdy_hm(stoptime))

# Recode the dates
trip_2 &amp;lt;- trip_2 %&amp;gt;% mutate(start_date = paste(month(start_dt), 
    day(start_dt), year(start_dt), sep = &amp;quot;/&amp;quot;))
trip_2$start_date &amp;lt;- mdy(trip_2$start_date)

trip_2 &amp;lt;- trip_2 %&amp;gt;% mutate(stop_date = paste(month(stop_dt), 
    day(stop_dt), year(stop_dt), sep = &amp;quot;/&amp;quot;))
trip_2$stop_date &amp;lt;- mdy(trip_2$stop_date)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Time to visualize the number of rides per day.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-19-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hmm, grouping by day is a little noisy. Perhaps we should try by month?&lt;/p&gt;

&lt;h4 id=&#34;plotting-trips-per-month-by-season&#34;&gt;Plotting Trips Per Month (By Season)&lt;/h4&gt;

&lt;p&gt;First, we need to create a &amp;ldquo;Year-Month&amp;rdquo; variable&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;start_date_ym &amp;lt;- trip_2 %&amp;gt;% mutate(ym = paste(year(start_date), 
    month(start_date), sep = &amp;quot;/&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now plot. I think I&amp;rsquo;ll plot this by month but color it by season (where December, January, and February are &amp;ldquo;winter&amp;rdquo;, March, April, and May are &amp;ldquo;spring&amp;rdquo;, June, July, August are &amp;ldquo;summer&amp;rdquo;, and September, October, November are &amp;ldquo;autumn&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-21-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Well that intuitively makes sense. The number of trips taken per month increases in the spring, reaches a maximum in the summer, declines through the fall, remains fairly stable in the winter and then repeats.&lt;/p&gt;

&lt;h4 id=&#34;average-trip-duration&#34;&gt;Average Trip Duration&lt;/h4&gt;

&lt;p&gt;Great! I wonder how the average trip duration fluctuates over this time period.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Convert Trip Duration from Seconds to Minutes
Trip_Duration_Month &amp;lt;- start_date_ym %&amp;gt;% mutate(trip_duration_min = tripduration/60) %&amp;gt;% 
    group_by(ym) %&amp;gt;% select(ym, trip_duration_min) %&amp;gt;% summarise(Avg = mean(trip_duration_min), 
    sd = sd(trip_duration_min)) %&amp;gt;% mutate(se = sd/sqrt(n()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to plot the average trip duration (in minutes) (plus or minus standard error), with colors indicating season.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-23-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s surprisingly not a huge range in trip durations here.&lt;/p&gt;

&lt;p&gt;The little bit of variation here makes logical sense. Longer trips were being taken in the spring and summer months rather than the fall and winter. It&amp;rsquo;s also notable that the spring and summer of 2016 may have shown fewer trips than the previous year, show a slight increase in average trip length.&lt;/p&gt;

&lt;h4 id=&#34;number-of-trips-by-day-of-week&#34;&gt;Number of Trips by Day of Week&lt;/h4&gt;

&lt;p&gt;I wonder if people are using this service to commute to/from work. Let&amp;rsquo;s look at the number of trips by day of the week.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s create a Day of the Week variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trip_2$wd &amp;lt;- wday(trip_2$start_date, label = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to plot the total number of trips by day of the week.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-25-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ok, so there are definitely more trips during the week than on the weekends. I wonder if this varies by season too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-26-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So it looks like usage is relatively consistent across seasons, at least as far as the number of trips are concerned.&lt;/p&gt;

&lt;h4 id=&#34;number-of-trips-by-time-of-day&#34;&gt;Number of Trips by Time of Day&lt;/h4&gt;

&lt;p&gt;How about time of day? Are people using these around commuting times during the week and later on weekends?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-27-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow, looks like regardless of the season, people are commuting to/from work using this service (there&amp;rsquo;s a spike between 8 and 10 AM and another between 4 and 7 PM Monday through Friday). But the weekends seem to be popular between 10 AM and 10 PM.&lt;/p&gt;

&lt;h4 id=&#34;number-of-trips-by-member-type&#34;&gt;Number of Trips by Member Type&lt;/h4&gt;

&lt;p&gt;I wonder if different types of members (those who have a membership vs. those that bought a 24 hour or 3 day pass) vary in the number of trips they take.&lt;/p&gt;

&lt;p&gt;If I were to guess, I&amp;rsquo;d think the short-term passes would be ideal for tourists or people looking for a quick weekend trip, whereas members may be more likely to continue using the service year-round. Let&amp;rsquo;s check out my assumptions by plotting, once again colored by season.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-28-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly (to me, at least), different types of users seem to follow similar patterns of usage. Spring and Summer are definitely the most popular times for anyone to ride a bike in the Seattle area.&lt;/p&gt;

&lt;h4 id=&#34;trip-duration-by-member-type&#34;&gt;Trip Duration by Member Type&lt;/h4&gt;

&lt;p&gt;While it may seem that the trip duration shouldn&amp;rsquo;t vary widely by member type, a quick look at &lt;a href=&#34;https://www.prontocycleshare.com/pricing&#34;&gt;Pronto!&amp;rsquo;s pricing structure&lt;/a&gt; may make you reconsider that assumption. You see, while you have to purchase either an annual membership ($85/year), a 24-Hour Pass ($8) or a 3-Day Pass ($16) there is still a cap on the duration of your trip. For members, any ride under 45 minutes is free, but any ride going over 45 minutes will incur a fee of $2 for every additional 30 minutes. For short-term users, any ride under 30 minutes is free, but going over that time limit would cost you an additional $2 for the first 30 minutes and $5 for each additional 30 minutes after that!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see if these time limits cause differing behaviors in our users.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-29-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ok, so our members are pretty good about making sure that they return their bike before they incur extra charges, but the short-term pass holders frequently go over their time limit. I wonder how the cost of a trip varies for members and pass holders. Let&amp;rsquo;s try to calculate the cost of a trip.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trip_cost &amp;lt;- trip_2 %&amp;gt;% mutate(cost = ifelse(usertype == &amp;quot;Member&amp;quot; &amp;amp; 
    tripduration_m &amp;lt;= 45, 0, ifelse(usertype == &amp;quot;Member&amp;quot; &amp;amp; tripduration_m &amp;gt; 
    45 &amp;amp; tripduration_m &amp;lt;= 75, 2, ifelse(usertype == &amp;quot;Member&amp;quot; &amp;amp; 
    tripduration_m &amp;gt; 75, (2 + 5 * ((tripduration_m - 75)/30)), 
    ifelse(usertype == &amp;quot;Short-Term Pass Holder&amp;quot; &amp;amp; tripduration_m &amp;lt;= 
        30, 0, ifelse(usertype == &amp;quot;Short-Term Pass Holder&amp;quot; &amp;amp; 
        tripduration_m &amp;gt; 30 &amp;amp; tripduration_m &amp;lt; 60, 2, ifelse(usertype == 
        &amp;quot;Short-Term Pass Holder&amp;quot; &amp;amp; tripduration_m &amp;gt; 60, (2 + 
        5 * ((tripduration_m - 60)/30)), &amp;quot;unknown&amp;quot;)))))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That was a complicated nested if/else statement! Let&amp;rsquo;s see how much these folks are paying in additional fees!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-31-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like short-term pass holders (who are already paying a higher price per day of biking), are also paying lots of extra fees. This could be because they are unfamiliar with the pricing structure and don&amp;rsquo;t realize they need to return their bike to a station within 30 minutes without getting charged. It is also possible that short-term users may be tourists who don&amp;rsquo;t know their way around as easily, and thus can&amp;rsquo;t find their way to a station within the time limit.&lt;/p&gt;

&lt;h4 id=&#34;member-demographics&#34;&gt;Member Demographics&lt;/h4&gt;

&lt;p&gt;We only seem to have age and gender information about people who have an annual Pronto! membership, so we can at least take a look at what types of people use this service.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look first at age.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trip_2$usertype &amp;lt;- as.factor(trip_2$usertype)
trip_age &amp;lt;- trip_2 %&amp;gt;% mutate(age = year(start_dt) - birthyear)

hist(trip_age$age, main = &amp;quot;Member Age&amp;quot;, xlab = &amp;quot;Number of Riders&amp;quot;, 
    col = &amp;quot;#56B4E9&amp;quot;, breaks = 25)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-32-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My first instinct here is to say &amp;ldquo;Wow! There&amp;rsquo;s a lot of 20 and 30-somethings that use this service!&amp;rdquo; But this figure (and these data) may be a little misleading. You see, we don&amp;rsquo;t have any sort of Rider ID number, meaning we can&amp;rsquo;t take &amp;ldquo;individual activity level&amp;rdquo; into account. So we can&amp;rsquo;t tell if the tallest spike is because 5 very athletic 28-year-olds went on 4,000 trips each, or if 100 people went on 200 trips each, or if there were 20,000 28-year-olds who each only used the service once.&lt;/p&gt;

&lt;p&gt;The same problem would arise if we looked at gender, so I&amp;rsquo;m just going to move beyond demographics.&lt;/p&gt;

&lt;h4 id=&#34;trip-routes&#34;&gt;Trip Routes&lt;/h4&gt;

&lt;p&gt;I&amp;rsquo;m going to do my best to look at some potential routes that these users could have taken, given their start and stop locations and duration. All of these data will be analyzed using the &lt;code&gt;ggmap&lt;/code&gt; package and Google Maps API (&lt;a href=&#34;https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf&#34;&gt;more info here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;To start, I need to combine the coordinates of the station (from the &lt;code&gt;station&lt;/code&gt; dataset) with the data from the &lt;code&gt;trip&lt;/code&gt; dataset. Let&amp;rsquo;s get started.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a dataframe with only station ID, latitude, and
# longitude
station_coord &amp;lt;- station %&amp;gt;% select(station_id, lat, long)

# Trim our trip dataframe to only include start &amp;amp; stop
# dates/times, and station ID
trip_route &amp;lt;- trip_2 %&amp;gt;% select(trip_id, starts_with(&amp;quot;start_&amp;quot;), 
    starts_with(&amp;quot;stop_&amp;quot;), from_station_id, to_station_id, tripduration)

# Match by station ID
trip_route$start_lat &amp;lt;- station_coord[match(trip_route$from_station_id, 
    station_coord$station_id), &amp;quot;lat&amp;quot;]

trip_route$start_long &amp;lt;- station_coord[match(trip_route$from_station_id, 
    station_coord$station_id), &amp;quot;long&amp;quot;]

trip_route$stop_lat &amp;lt;- station_coord[match(trip_route$to_station_id, 
    station_coord$station_id), &amp;quot;lat&amp;quot;]

trip_route$stop_long &amp;lt;- station_coord[match(trip_route$to_station_id, 
    station_coord$station_id), &amp;quot;long&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Now to start looking at routes.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll start by looking at the possible routes of the very first trip.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-34-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cool! So Google Maps API was able to give us two potential routes for this particular trip. We can make a best guess on which trip was taken by determining the trip duration.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Converting trip duration to minutes
trip_route$tripduration &amp;lt;- trip_route$tripduration/60

# Finding actual trip duration
trip_route[1, &amp;quot;tripduration&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 16.43225
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so the actual trip on October 13, 2014 took 16.4 minutes. Let&amp;rsquo;s see how long each of the hypothetical trips took.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;leg_1 %&amp;gt;% group_by(route) %&amp;gt;% summarise(duration = sum(minutes))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 2
##   route duration
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     A 4.950000
## 2     B 5.650000
## 3     C 5.483333
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hmm, looks like Google estimates those trips should have only taken between 5 and 6 minutes. It is very possible that our user did not go directly from one station to the other. It&amp;rsquo;s also possible that he got stopped at a few red lights along his route.&lt;/p&gt;

&lt;p&gt;Perhaps this, being the first trip, was a demo of some sort. Let&amp;rsquo;s check out the last trip that was recorded and see if we still run into the same time disconnect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-37-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt; Well this was certainly a longer ride than the first one we looked at!&lt;/p&gt;

&lt;p&gt;How long did this trip take?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;trip_route[nrow(trip_route), &amp;quot;tripduration&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 31.60052
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so how long did Google estimate it should take?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;leg_2 %&amp;gt;% group_by(route) %&amp;gt;% summarise(duration = sum(minutes))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 2
##   route duration
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     A  9.50000
## 2     B 12.13333
## 3     C 13.45000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once again, the trip took quite a bit longer than any of the routes shown here. It&amp;rsquo;s possible that estimating the routes taken by cyclists is not the best use of this dataset. So, what else is there to look at?&lt;/p&gt;

&lt;h4 id=&#34;station-by-trip-departure-and-arrival&#34;&gt;Station by Trip Departure and arrival&lt;/h4&gt;

&lt;p&gt;I suppose we can see which stations have the most trip departures and which have the most arrivals. Unless people are always making round-trip journeys, these are not likely to be equal.&lt;/p&gt;

&lt;h4 id=&#34;station-usage&#34;&gt;Station Usage&lt;/h4&gt;

&lt;h5 id=&#34;trip-departure&#34;&gt;Trip Departure&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-40-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Great, so it looks like the station that sends the highest number of bikes out and takes the highest number of bikes in is located at Pier 69 / Alaskan Way &amp;amp; Clay Street. Looks like this is pretty close to a few parks and several big tourist attractions.&lt;/p&gt;

&lt;p&gt;Also, if you flip between &amp;ldquo;Departures&amp;rdquo; and &amp;ldquo;Arrivals&amp;rdquo;, you&amp;rsquo;ll notice that quite a few bikes are picked up in the Capitol Hill area, but are returned down by the coast. If you&amp;rsquo;re unfamiliar with Seattle&amp;rsquo;s topography, Capitol Hill is aptly named because it&amp;rsquo;s situated on a very steep hill. It makes sense that people would borrow bikes to ride &lt;em&gt;down&lt;/em&gt; the hill, but not want to borrow a bike to go back &lt;em&gt;up&lt;/em&gt; the hill. Interesting!&lt;/p&gt;

&lt;h5 id=&#34;trip-arrival&#34;&gt;Trip Arrival&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-41-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Great, so it looks like the station that sends the highest number of bikes out and takes the highest number of bikes in is located at Pier 69 / Alaskan Way &amp;amp; Clay Street. Looks like this is pretty close to a few parks and several big tourist attractions.&lt;/p&gt;

&lt;p&gt;Also, if you flip between &amp;ldquo;Departures&amp;rdquo; and &amp;ldquo;Arrivals&amp;rdquo;, you&amp;rsquo;ll notice that quite a few bikes are picked up in the Capitol Hill area, but are returned down by the coast. If you&amp;rsquo;re unfamiliar with Seattle&amp;rsquo;s topography, Capitol Hill is aptly named because it&amp;rsquo;s situated on a very steep hill. It makes sense that people would borrow bikes to ride &lt;em&gt;down&lt;/em&gt; the hill, but not want to borrow a bike to go back &lt;em&gt;up&lt;/em&gt; the hill. Interesting!&lt;/p&gt;

&lt;h3 id=&#34;exploring-the-weather-dataset&#34;&gt;Exploring the Weather Dataset&lt;/h3&gt;

&lt;p&gt;Now that I&amp;rsquo;ve visualized all that I can think of in terms of the &amp;ldquo;trips&amp;rdquo; dataset, it&amp;rsquo;s time to take a brief look at the weather dataset.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s get a quick reminder of what we&amp;rsquo;re looking at here.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;glimpse(weather)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Observations: 689
## Variables: 21
## $ Date                       &amp;lt;chr&amp;gt; &amp;quot;10/13/2014&amp;quot;, &amp;quot;10/14/2014&amp;quot;, &amp;quot;10/15/...
## $ Max_Temperature_F          &amp;lt;int&amp;gt; 71, 63, 62, 71, 64, 68, 73, 66, 64,...
## $ Mean_Temperature_F         &amp;lt;int&amp;gt; 62, 59, 58, 61, 60, 64, 64, 60, 58,...
## $ Min_TemperatureF           &amp;lt;int&amp;gt; 54, 55, 54, 52, 57, 59, 55, 55, 55,...
## $ Max_Dew_Point_F            &amp;lt;int&amp;gt; 55, 52, 53, 49, 55, 59, 57, 57, 52,...
## $ MeanDew_Point_F            &amp;lt;int&amp;gt; 51, 51, 50, 46, 51, 57, 55, 54, 49,...
## $ Min_Dewpoint_F             &amp;lt;int&amp;gt; 46, 50, 46, 42, 41, 55, 53, 50, 46,...
## $ Max_Humidity               &amp;lt;int&amp;gt; 87, 88, 87, 83, 87, 90, 94, 90, 87,...
## $ Mean_Humidity              &amp;lt;int&amp;gt; 68, 78, 77, 61, 72, 83, 74, 78, 70,...
## $ Min_Humidity               &amp;lt;int&amp;gt; 46, 63, 67, 36, 46, 68, 52, 67, 58,...
## $ Max_Sea_Level_Pressure_In  &amp;lt;dbl&amp;gt; 30.03, 29.84, 29.98, 30.03, 29.83, ...
## $ Mean_Sea_Level_Pressure_In &amp;lt;dbl&amp;gt; 29.79, 29.75, 29.71, 29.95, 29.78, ...
## $ Min_Sea_Level_Pressure_In  &amp;lt;dbl&amp;gt; 29.65, 29.54, 29.51, 29.81, 29.73, ...
## $ Max_Visibility_Miles       &amp;lt;int&amp;gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,...
## $ Mean_Visibility_Miles      &amp;lt;int&amp;gt; 10, 9, 9, 10, 10, 8, 10, 10, 10, 6,...
## $ Min_Visibility_Miles       &amp;lt;int&amp;gt; 4, 3, 3, 10, 6, 2, 6, 5, 6, 2, 10, ...
## $ Max_Wind_Speed_MPH         &amp;lt;int&amp;gt; 13, 10, 18, 9, 8, 10, 10, 12, 15, 1...
## $ Mean_Wind_Speed_MPH        &amp;lt;int&amp;gt; 4, 5, 7, 4, 3, 4, 3, 5, 8, 8, 9, 4,...
## $ Max_Gust_Speed_MPH         &amp;lt;chr&amp;gt; &amp;quot;21&amp;quot;, &amp;quot;17&amp;quot;, &amp;quot;25&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;-&amp;quot;, &amp;quot;1...
## $ Precipitation_In           &amp;lt;dbl&amp;gt; 0.00, 0.11, 0.45, 0.00, 0.14, 0.31,...
## $ Events                     &amp;lt;chr&amp;gt; &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Rain&amp;quot;, &amp;quot;Ra...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great, let&amp;rsquo;s change the Date variable to a POSIXct object, and make the &amp;ldquo;Events&amp;rdquo; variable factors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Adjusting the Date Variable
weather$Date &amp;lt;- mdy(weather$Date)

# Adjusting the Events Variable
weather$Events &amp;lt;- as.factor(weather$Events)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great. Now how many weather events are there?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;levels(weather$Events)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;&amp;quot;                    &amp;quot;Fog&amp;quot;                 &amp;quot;Fog , Rain&amp;quot;         
##  [4] &amp;quot;Fog-Rain&amp;quot;            &amp;quot;Rain&amp;quot;                &amp;quot;Rain , Snow&amp;quot;        
##  [7] &amp;quot;Rain , Thunderstorm&amp;quot; &amp;quot;Rain-Snow&amp;quot;           &amp;quot;Rain-Thunderstorm&amp;quot;  
## [10] &amp;quot;Snow&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow! So mostly combinations of rain&amp;hellip;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s combine a few of these things that seem to represent the same event.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weather$Events &amp;lt;- gsub(&amp;quot;Fog , Rain|Fog-Rain&amp;quot;, &amp;quot;Fog-Rain&amp;quot;, weather$Events)
weather$Events &amp;lt;- gsub(&amp;quot;Rain , Snow|Rain-Snow&amp;quot;, &amp;quot;Rain-Snow&amp;quot;, 
    weather$Events)
weather$Events &amp;lt;- gsub(&amp;quot;Rain , Thunderstorm|Rain-Thunderstorm&amp;quot;, 
    &amp;quot;Rain-TS&amp;quot;, weather$Events)

weather$Events &amp;lt;- as.factor(weather$Events)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where else does this dataset need to be cleaned up? Let&amp;rsquo;s look for any missing values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(weather)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##       Date            Max_Temperature_F Mean_Temperature_F
##  Min.   :2014-10-13   Min.   :39.00     Min.   :33.00     
##  1st Qu.:2015-04-03   1st Qu.:55.00     1st Qu.:48.00     
##  Median :2015-09-22   Median :63.00     Median :56.00     
##  Mean   :2015-09-22   Mean   :64.03     Mean   :56.58     
##  3rd Qu.:2016-03-12   3rd Qu.:73.00     3rd Qu.:65.00     
##  Max.   :2016-08-31   Max.   :98.00     Max.   :83.00     
##                                         NA&#39;s   :1         
##  Min_TemperatureF Max_Dew_Point_F MeanDew_Point_F Min_Dewpoint_F 
##  Min.   :23.00    Min.   :10.00   Min.   : 4.00   Min.   : 1.00  
##  1st Qu.:43.00    1st Qu.:44.00   1st Qu.:41.00   1st Qu.:36.00  
##  Median :50.00    Median :50.00   Median :46.00   Median :42.00  
##  Mean   :49.45    Mean   :48.57   Mean   :45.02   Mean   :40.87  
##  3rd Qu.:57.00    3rd Qu.:54.00   3rd Qu.:51.00   3rd Qu.:47.00  
##  Max.   :70.00    Max.   :77.00   Max.   :59.00   Max.   :57.00  
##                                                                  
##   Max_Humidity    Mean_Humidity    Min_Humidity  
##  Min.   : 40.00   Min.   :24.00   Min.   :15.00  
##  1st Qu.: 78.00   1st Qu.:60.00   1st Qu.:38.00  
##  Median : 86.00   Median :70.00   Median :50.00  
##  Mean   : 84.54   Mean   :68.51   Mean   :49.97  
##  3rd Qu.: 90.00   3rd Qu.:79.00   3rd Qu.:63.00  
##  Max.   :100.00   Max.   :95.00   Max.   :87.00  
##                                                  
##  Max_Sea_Level_Pressure_In Mean_Sea_Level_Pressure_In
##  Min.   :29.47             Min.   :29.31             
##  1st Qu.:30.01             1st Qu.:29.93             
##  Median :30.12             Median :30.04             
##  Mean   :30.12             Mean   :30.03             
##  3rd Qu.:30.24             3rd Qu.:30.16             
##  Max.   :30.86             Max.   :30.81             
##                                                      
##  Min_Sea_Level_Pressure_In Max_Visibility_Miles Mean_Visibility_Miles
##  Min.   :29.14             Min.   : 3.00        Min.   : 1.00        
##  1st Qu.:29.84             1st Qu.:10.00        1st Qu.: 9.00        
##  Median :29.96             Median :10.00        Median :10.00        
##  Mean   :29.94             Mean   : 9.99        Mean   : 9.43        
##  3rd Qu.:30.08             3rd Qu.:10.00        3rd Qu.:10.00        
##  Max.   :30.75             Max.   :10.00        Max.   :10.00        
##                                                                      
##  Min_Visibility_Miles Max_Wind_Speed_MPH Mean_Wind_Speed_MPH
##  Min.   : 0.000       Min.   : 4.00      Min.   : 0.000     
##  1st Qu.: 4.000       1st Qu.: 8.00      1st Qu.: 3.000     
##  Median : 9.000       Median :10.00      Median : 4.000     
##  Mean   : 7.245       Mean   :11.09      Mean   : 4.631     
##  3rd Qu.:10.000       3rd Qu.:13.00      3rd Qu.: 6.000     
##  Max.   :10.000       Max.   :30.00      Max.   :23.000     
##                                                             
##  Max_Gust_Speed_MPH Precipitation_In       Events   
##  Length:689         Min.   :0.0000            :361  
##  Class :character   1st Qu.:0.0000   Fog      : 16  
##  Mode  :character   Median :0.0000   Fog-Rain : 13  
##                     Mean   :0.1051   Rain     :287  
##                     3rd Qu.:0.0900   Rain-Snow:  3  
##                     Max.   :2.2000   Rain-TS  :  7  
##                                      Snow     :  2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so we have one NA for &amp;ldquo;Mean_Temperature_F&amp;rdquo;, &amp;ldquo;Max_Gust_Speed_MPH&amp;rdquo; seems to be represented as a character vector because it has &amp;ldquo;-&amp;rdquo; representing NA values, and we have 361 unlabelled Events.&lt;/p&gt;

&lt;p&gt;Max Gust Speed should be the easiest one to fix, so we&amp;rsquo;ll start there.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weather$Max_Gust_Speed_MPH &amp;lt;- gsub(&amp;quot;-&amp;quot;, 0, weather$Max_Gust_Speed_MPH)

weather$Max_Gust_Speed_MPH &amp;lt;- as.numeric(weather$Max_Gust_Speed_MPH)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! We changed any absent values for Maximum Gust Speed to 0 MPH and changed the variable type to a number. Uh oh, looks like there are still 185 NA values for Max Gust Speed. That&amp;rsquo;s a lot to try to replace. I would normally suggest generating a model that could try to predict those values based on other known values, but for now, we&amp;rsquo;ll just leave it alone.&lt;/p&gt;

&lt;p&gt;Since there is only one missing Mean Temperature, it seems the easiest way to fill in the hole is to look up what the average temperature was that day. &lt;em&gt;Note: I certainly would not recommend this if it were any more than one missing value&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weather[which(is.na(weather$Mean_Temperature_F)), 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2016-02-14&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so we&amp;rsquo;re looking for the Mean Temperature on February 14, 2016 in the zipcode 98101 (according to dataset documentation). Looks like the mean temperature that day was 50 degrees F.&lt;/p&gt;

&lt;p&gt;Time to substitute in that value.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weather[490, &amp;quot;Mean_Temperature_F&amp;quot;] &amp;lt;- &amp;quot;50&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Perfect. Now what to do with the unlabelled &amp;ldquo;Event&amp;rdquo; categories. The dataset &amp;ldquo;ReadMe&amp;rdquo; file from Pronto! doesn&amp;rsquo;t include any information about this weather dataset. The only thing I can think to do is refer to the Event as &amp;ldquo;Other&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;weather$Events &amp;lt;- gsub(&amp;quot;^$&amp;quot;, &amp;quot;Other&amp;quot;, weather$Events)
weather$Events &amp;lt;- as.factor(weather$Events)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, we&amp;rsquo;re in good shape. Now to do a few quick visualizations.&lt;/p&gt;

&lt;h4 id=&#34;temperature&#34;&gt;Temperature&lt;/h4&gt;

&lt;h5 id=&#34;minimum&#34;&gt;Minimum&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-51-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;mean&#34;&gt;Mean&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-52-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;maximum&#34;&gt;Maximum&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-53-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;events&#34;&gt;Events&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-54-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;combining-weather-and-trip-datasets&#34;&gt;Combining Weather and Trip Datasets&lt;/h4&gt;

&lt;p&gt;Good, so we can now see some parts of the weather data. Let&amp;rsquo;s combine the weather data with our trip data. Let&amp;rsquo;s try a &lt;code&gt;left join&lt;/code&gt; from the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make a copy of the data frame
trip_3 &amp;lt;- trip_2

# Change column name in trip_3 to match weather dataset
trip_3$Date &amp;lt;- trip_3$start_date

# Left join the trip and weather dataframes by date.
trip_weather &amp;lt;- left_join(trip_3, weather, by = &amp;quot;Date&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;mean-temperature-vs-number-of-trips&#34;&gt;Mean Temperature vs. Number of Trips&lt;/h4&gt;

&lt;p&gt;Ok. Now let&amp;rsquo;s see how the number of trips per day is influenced by weather (mean temperature, rounded to the nearest 5 degrees F)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-56-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So, as expected, there are more trips when the weather is mild but not too warm (over 70F) or too cold (below 50F). However, this figure may be influenced by the overall number of days that exhibited each mean temperature. Let&amp;rsquo;s try to standardize that.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-57-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So when we standardize our measurements, correcting for the number of days that actually reached each temperature, we see a steady increase in the number of trips until around 75F where the trend levels off. People are more likely to ride a bike when it&amp;rsquo;s warm outside.&lt;/p&gt;

&lt;h4 id=&#34;precipitation-vs-number-of-trips&#34;&gt;Precipitation vs. Number of Trips&lt;/h4&gt;

&lt;p&gt;If you&amp;rsquo;ve ever heard of Seattle, you probably hear that it rains all the time there. Let&amp;rsquo;s see if that has an impact on the number of trips taken in a day.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll start with a figure standardized for number of days at a precipitation level, rounded to the nearest 0.2 inches. &lt;img src=&#34;Bicycle_Sharing_2_files/figure-markdown_github/unnamed-chunk-58-1.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like even Seattleites have a limit when it comes to riding a bike in the rain. The more it rained, the fewer trips were taken per day.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;So what did we learn from all of this? In the nearly 2 years since Pronto! opened in Seattle:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;236,065 bike trips were taken using this service&lt;/li&gt;
&lt;li&gt;More trips occur in the spring and summer than winter/autumn&lt;/li&gt;
&lt;li&gt;More trips occur during warm/dry weather&lt;/li&gt;
&lt;li&gt;People tend to ride downhill more frequently than uphill&lt;/li&gt;
&lt;li&gt;Pronto! bikes are used for work commutes during the week and more leisurely use on weekends&lt;/li&gt;
&lt;li&gt;Short-Term Pass Holders are more likely to incur extra charges due to surpassing their time limit&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;suggestions-for-pronto&#34;&gt;Suggestions for Pronto!&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Give users bonuses for bringing bikes back to a station on the top of the hill&lt;/li&gt;
&lt;li&gt;Hold discounts in fall/winter&lt;/li&gt;
&lt;li&gt;Find a way to alert short-term users that their time limit will be ending soon, and where the nearest station is to them at that time&lt;/li&gt;
&lt;li&gt;Consider a 3rd membership option: &amp;ldquo;Commuter&amp;rdquo;. This may allow users to take bikes between 7-10 AM and 4-7 PM for free, but operate under a different time limit schedule during other times of day.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, I appreciate any and all feedback from my work and appreciate you taking the time to see what I&amp;rsquo;ve done. Thanks!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kaggle - Ghosts, Goblins, and Ghouls</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Ghosts_Goblins_Ghouls_01/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Ghosts_Goblins_Ghouls_01/</guid>
      <description>&lt;p&gt;Data exploration and machine learning in RMarkdown.
&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is my second-ever Kaggle competition (looking for the &lt;a href=&#34;https://www.kaggle.com/amberthomas/titanic/predicting-survival-on-the-titanic&#34;&gt;first&lt;/a&gt;?) I&amp;rsquo;ll do my best to walk through my thought-process here and welcome any comments on my work. Let&amp;rsquo;s get started!&lt;/p&gt;

&lt;h3 id=&#34;loading-necessary-packages&#34;&gt;Loading Necessary Packages&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# For data manipulation and tidying
library(dplyr)

# For data visualizations
library(ggplot2)
library(fpc)

# For modeling and predictions
library(caret)
library(glmnet)
library(ranger)
library(e1071)
library(clValid)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h3&gt;

&lt;p&gt;The data were downloaded directly from the &lt;a href=&#34;https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo/data&#34;&gt;Kaggle Website&lt;/a&gt;. Before binding the training and test sets into a single data file, I added a column called &amp;ldquo;Dataset&amp;rdquo; and labelled rows from the training file &amp;ldquo;train&amp;rdquo; and rows from the testing file &amp;ldquo;test&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train &amp;lt;- read.csv(file = &amp;quot;train.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)
train$Dataset &amp;lt;- &amp;quot;train&amp;quot;

test &amp;lt;- read.csv(file = &amp;quot;test.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)
test$Dataset &amp;lt;- &amp;quot;test&amp;quot;

full &amp;lt;- bind_rows(train, test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, time to take a look at the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(full)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:    900 obs. of  8 variables:
##  $ id           : int  0 1 2 4 5 7 8 11 12 19 ...
##  $ bone_length  : num  0.355 0.576 0.468 0.777 0.566 ...
##  $ rotting_flesh: num  0.351 0.426 0.354 0.509 0.876 ...
##  $ hair_length  : num  0.466 0.531 0.812 0.637 0.419 ...
##  $ has_soul     : num  0.781 0.44 0.791 0.884 0.636 ...
##  $ color        : chr  &amp;quot;clear&amp;quot; &amp;quot;green&amp;quot; &amp;quot;black&amp;quot; &amp;quot;black&amp;quot; ...
##  $ type         : chr  &amp;quot;Ghoul&amp;quot; &amp;quot;Goblin&amp;quot; &amp;quot;Ghoul&amp;quot; &amp;quot;Ghoul&amp;quot; ...
##  $ Dataset      : chr  &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(full)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##        id         bone_length     rotting_flesh     hair_length    
##  Min.   :  0.0   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:224.8   1st Qu.:0.3321   1st Qu.:0.4024   1st Qu.:0.3961  
##  Median :449.5   Median :0.4268   Median :0.5053   Median :0.5303  
##  Mean   :449.5   Mean   :0.4291   Mean   :0.5050   Mean   :0.5222  
##  3rd Qu.:674.2   3rd Qu.:0.5182   3rd Qu.:0.6052   3rd Qu.:0.6450  
##  Max.   :899.0   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##     has_soul         color               type             Dataset         
##  Min.   :0.0000   Length:900         Length:900         Length:900        
##  1st Qu.:0.3439   Class :character   Class :character   Class :character  
##  Median :0.4655   Mode  :character   Mode  :character   Mode  :character  
##  Mean   :0.4671                                                           
##  3rd Qu.:0.5892                                                           
##  Max.   :1.0000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! So here&amp;rsquo;s what we know so far:&lt;/p&gt;

&lt;p&gt;We have 8 variables currently:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt; : Appears to be the identification number of the monster in question&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bone Length&lt;/strong&gt; : Average length of the bones in the creature, normalized to 0 - 1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rotting Flesh&lt;/strong&gt; : Percentage of flesh on the creature that is rotting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hair Length&lt;/strong&gt; : Average length of the hair on the creature, normalized from 0 - 1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Has Soul&lt;/strong&gt; : The percentage of a soul present in the creature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Color&lt;/strong&gt; : The color of the creature&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt; : The category of the creature (i.e. ghoul, goblin or ghost)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt; : The column I added when importing data indicating whether the observation was part of the original training or test set&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It seems like a few of these variables would serve better as factors, rather than character strings, so I&amp;rsquo;ll take care of that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;factor_variables &amp;lt;- c(&#39;id&#39;, &#39;color&#39;, &#39;type&#39;, &#39;Dataset&#39;)
full[factor_variables] &amp;lt;- lapply(full[factor_variables], function(x) as.factor(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;data-exploration&#34;&gt;Data Exploration&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at what we&amp;rsquo;ve got here so far. What&amp;rsquo;s the distribution of each variable across each monster?&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s first temporarily remove the &amp;ldquo;test&amp;rdquo; rows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_2 &amp;lt;- full[full$Dataset == &#39;train&#39;, ]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;distribution-of-continuous-variables-by-creature-type&#34;&gt;Distribution of Continuous Variables by Creature Type&lt;/h3&gt;

&lt;h4 id=&#34;bone-length&#34;&gt;Bone Length&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-6-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;rotting-flesh&#34;&gt;Rotting Flesh&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-7-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;hair-length&#34;&gt;Hair Length&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-8-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;soul&#34;&gt;Soul&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-9-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distribution-of-color-by-creature-type&#34;&gt;Distribution of Color by Creature Type&lt;/h3&gt;

&lt;h4 id=&#34;ghost&#34;&gt;Ghost&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-10-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;ghoul&#34;&gt;Ghoul&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;goblin&#34;&gt;Goblin&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-12-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;distinguishing-features&#34;&gt;Distinguishing Features?&lt;/h3&gt;

&lt;p&gt;Hmm, looks like ghosts have shorter hair and fewer pieces of soul than ghouls and goblins, but otherwise are pretty close. Ghouls and goblins are going to be tricky to distinguish. Color doesn&amp;rsquo;t appear to help a whole lot as there seems to be a pretty even distribution to these multi-colored critters.&lt;/p&gt;

&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;

&lt;p&gt;Normally here I would try to come up with additional ways to look at these data, but we can&amp;rsquo;t infer the size of the creature since both bone and hair length have been normalized. As of now, I can&amp;rsquo;t think of any features worth engineering from the current data.&lt;/p&gt;

&lt;p&gt;Maybe I&amp;rsquo;m missing some interesting connection between variables?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs(full[,2:5], 
      col = full$type, 
      labels = c(&amp;quot;Bone Length&amp;quot;, &amp;quot;Rotting Flesh&amp;quot;, &amp;quot;Hair Length&amp;quot;, &amp;quot;Soul&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-13-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Nope. But perhaps we can take advantage of a combination of characteristics that do seem to show some promise: most notably &amp;ldquo;Hair Length&amp;rdquo; and &amp;ldquo;Soul&amp;rdquo;. Do we get any better separation among creatures if we combine these variables into one?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full &amp;lt;- full %&amp;gt;%
          mutate(hair_soul = hair_length * has_soul)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-15-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That may have separated Ghosts a little further from the other two&amp;hellip; Let&amp;rsquo;s try a few more variable interactions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full &amp;lt;- full %&amp;gt;%
          mutate(bone_flesh = bone_length * rotting_flesh,
                 bone_hair = bone_length * hair_length,
                 bone_soul = bone_length * has_soul,
                 flesh_hair = rotting_flesh * hair_length,
                 flesh_soul = rotting_flesh * has_soul)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to check for ways to tidy up.&lt;/p&gt;

&lt;h2 id=&#34;cleaning-data&#34;&gt;Cleaning Data&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s take another look at the summary statistics for this dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(full)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##        id       bone_length     rotting_flesh     hair_length    
##  0      :  1   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1      :  1   1st Qu.:0.3321   1st Qu.:0.4024   1st Qu.:0.3961  
##  2      :  1   Median :0.4268   Median :0.5053   Median :0.5303  
##  3      :  1   Mean   :0.4291   Mean   :0.5050   Mean   :0.5222  
##  4      :  1   3rd Qu.:0.5182   3rd Qu.:0.6052   3rd Qu.:0.6450  
##  5      :  1   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  (Other):894                                                     
##     has_soul        color         type      Dataset      hair_soul     
##  Min.   :0.0000   black:104   Ghost :117   test :529   Min.   :0.0000  
##  1st Qu.:0.3439   blood: 21   Ghoul :129   train:371   1st Qu.:0.1322  
##  Median :0.4655   blue : 54   Goblin:125               Median :0.2448  
##  Mean   :0.4671   clear:292   NA&#39;s  :529               Mean   :0.2588  
##  3rd Qu.:0.5892   green: 95                            3rd Qu.:0.3631  
##  Max.   :1.0000   white:334                            Max.   :0.7768  
##                                                                        
##    bone_flesh       bone_hair        bone_soul        flesh_hair    
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.1473   1st Qu.:0.1361   1st Qu.:0.1136   1st Qu.:0.1847  
##  Median :0.2039   Median :0.2194   Median :0.1944   Median :0.2473  
##  Mean   :0.2159   Mean   :0.2330   Mean   :0.2098   Mean   :0.2585  
##  3rd Qu.:0.2701   3rd Qu.:0.3191   3rd Qu.:0.2810   3rd Qu.:0.3242  
##  Max.   :0.7887   Max.   :0.7779   Max.   :0.6869   Max.   :0.7478  
##                                                                     
##    flesh_soul    
##  Min.   :0.0000  
##  1st Qu.:0.1539  
##  Median :0.2163  
##  Mean   :0.2316  
##  3rd Qu.:0.2991  
##  Max.   :0.7195  
## 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only column that has any missing values is &lt;code&gt;type&lt;/code&gt; which is to be expected since that&amp;rsquo;s what we need to be predicting. Everything else seems to look good so far. Let&amp;rsquo;s try to model these as is.&lt;/p&gt;

&lt;h2 id=&#34;clustering-data&#34;&gt;Clustering data&lt;/h2&gt;

&lt;p&gt;While clustering is generally used for unsupervised machine learning, I want to take a peek at the clusters that could be formed using the data at hand. The potential issue with trying to cluster this data is that we are working with two types of data: continuous and categorical. They break down like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Continuous Variables&lt;/th&gt;
&lt;th&gt;Categorical Variables&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;bone length&lt;/td&gt;
&lt;td&gt;id&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;rotting flesh&lt;/td&gt;
&lt;td&gt;color&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hair length&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;has soul&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So, sure, there&amp;rsquo;s only two categorical variables. Because of our small sample size, it&amp;rsquo;s not a good idea to count out these variables completely, but we&amp;rsquo;ll try to create clusters without them just to see how well the clustering models do.&lt;/p&gt;

&lt;h3 id=&#34;cluster-without-categorical-variables&#34;&gt;Cluster Without Categorical Variables&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ll first try to cluster using the &lt;code&gt;kmeans&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Set the seed
set.seed(100)

# Extract creature labels and remove column from dataset
creature_labels &amp;lt;- full$type
full2 &amp;lt;- full
full2$type &amp;lt;- NULL

# Remove categorical variables (id, color, and dataset) from dataset
full2$id &amp;lt;- NULL
full2$color &amp;lt;- NULL
full2$Dataset &amp;lt;- NULL

# Perform k-means clustering with 3 clusters, repeat 30 times
creature_km_1 &amp;lt;- kmeans(full2, 3, nstart = 30)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so now we have clusters, time to see how well they did. Let&amp;rsquo;s look at them graphically first. This was created using the &lt;code&gt;plotcluster()&lt;/code&gt; function from the &lt;code&gt;fpc&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-19-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hmm, those clusters don&amp;rsquo;t look very discrete. Let&amp;rsquo;s look at &lt;a href=&#34;https://en.wikipedia.org/wiki/Dunn_index&#34;&gt;Dunn&amp;rsquo;s Index&lt;/a&gt; mathematically to see if we&amp;rsquo;re missing something visually. This calculation comes from the &lt;code&gt;dunn&lt;/code&gt; function in the &lt;code&gt;clValid&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dunn_ckm_1 &amp;lt;- dunn(clusters = creature_km_1$cluster, Data = full2)

# Print results
dunn_ckm_1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 0.04670431
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As Dunn&amp;rsquo;s Index represents a ratio of the smallest distance between clusters to the largest distance between two points in the same cluster (or, the smallest inter-cluster distance to the largest intra-cluster distance), such a low number indicates that our current clusters are not condensed, separate entities. This is not terribly surprising considering we completely disregarded one of our variables.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how well this clustering method correctly separated the labelled creatures.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(creature_km_1$cluster, creature_labels)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    creature_labels
##     Ghost Ghoul Goblin
##   1     7    39     75
##   2     4    86     24
##   3   106     4     26
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like currently, ghosts were separated relatively well, but ghouls and goblins are split between the clusters. Ok, I&amp;rsquo;m convinced. I haven&amp;rsquo;t really gained any new information here, but it&amp;rsquo;s been an interesting exploratory path!&lt;/p&gt;

&lt;p&gt;On to supervised modeling!&lt;/p&gt;

&lt;h3 id=&#34;modeling-for-creature-identity&#34;&gt;Modeling for Creature Identity&lt;/h3&gt;

&lt;p&gt;Clustering was not particularly helpful in discerning creature identity, so perhaps creating models will work better.&lt;/p&gt;

&lt;p&gt;First things first, I need to split out the test and training data back into separate datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_complete &amp;lt;- full[full$Dataset == &#39;train&#39;, ]
test_complete &amp;lt;- full[full$Dataset == &#39;test&#39;, ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because I plan on using the &lt;code&gt;caret&lt;/code&gt; package for all of my modeling, I&amp;rsquo;m going to generate a standard &lt;code&gt;trainControl&lt;/code&gt; so that those tuning parameters remain consistent throughout the various models.&lt;/p&gt;

&lt;h3 id=&#34;creating-traincontrol&#34;&gt;Creating trainControl&lt;/h3&gt;

&lt;p&gt;I will create a system that will perform 20 repeats of a 10-Fold cross-validation of the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;myControl &amp;lt;- trainControl(
      method = &amp;quot;cv&amp;quot;, 
      number = 10,
      repeats = 20, 
      verboseIter = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;random-forest-modeling&#34;&gt;Random Forest Modeling&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s start with a random forest model, generated using the &lt;code&gt;ranger&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt; packages. I&amp;rsquo;m going to include all of the original variables, including any interactions here.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(10)

rf_model &amp;lt;- train(
    type ~ bone_length + rotting_flesh + hair_length + has_soul + color + hair_soul + bone_flesh + bone_hair + 
        bone_soul + flesh_hair + flesh_soul,
    tuneLength = 3,
    data = train_complete, 
    method = &amp;quot;ranger&amp;quot;, 
    trControl = myControl,
    importance = &#39;impurity&#39;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at the levels of importance of each factor in this model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-25-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Huh. Our &amp;ldquo;hair_soul&amp;rdquo; variable seems to be the most important to this model and our other interactions rank pretty highly. I suppose we can hold on to them for now. Color, on the other hand, hardly plays into this. Let&amp;rsquo;s try removing it from a second random forest model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(10)

rf_model_2 &amp;lt;- train(
    type ~ bone_length + rotting_flesh + hair_length + has_soul + hair_soul + bone_flesh + bone_hair + 
        bone_soul + flesh_hair + flesh_soul,
    tuneLength = 3,
    data = train_complete, 
    method = &amp;quot;ranger&amp;quot;, 
    trControl = myControl,
    importance = &#39;impurity&#39;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;glmnet-modeling&#34;&gt;GLMnet Modeling&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m going to follow the random forest model up with a glmnet model, also from the &lt;code&gt;caret&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(10)

glm_model &amp;lt;- train(
    type ~ bone_length + rotting_flesh + hair_length + has_soul + color + hair_soul + bone_flesh + bone_hair + 
        bone_soul + flesh_hair + flesh_soul, 
    method = &amp;quot;glmnet&amp;quot;,
    tuneGrid = expand.grid(alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)),
    data = train_complete,
    trControl = myControl
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once again, we&amp;rsquo;ll try without &amp;ldquo;color&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(10)

glm_model_2 &amp;lt;- train(
    type ~ bone_length + rotting_flesh + hair_length + has_soul + hair_soul + bone_flesh + bone_hair + 
        bone_soul + flesh_hair + flesh_soul, 
    method = &amp;quot;glmnet&amp;quot;,
    tuneGrid = expand.grid(alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)),
    data = train_complete,
    trControl = myControl
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;comparing-model-fit&#34;&gt;Comparing model fit&lt;/h3&gt;

&lt;p&gt;Now that we have two random forest models and two glmnet models, it&amp;rsquo;s time to compare their fit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a list of models
models &amp;lt;- list(rf = rf_model, rf2 = rf_model_2, glmnet = glm_model, glmnet2 = glm_model_2)

# Resample the models
resampled &amp;lt;- resamples(models)

# Generate a summary
summary(resampled)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## summary.resamples(object = resampled)
## 
## Models: rf, rf2, glmnet, glmnet2 
## Number of resamples: 10 
## 
## Accuracy 
##           Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## rf      0.6389  0.6888 0.7201 0.7194  0.7566 0.7838    0
## rf2     0.6667  0.6888 0.7105 0.7195  0.7500 0.7895    0
## glmnet  0.6842  0.7222 0.7333 0.7438  0.7616 0.8649    0
## glmnet2 0.6842  0.7047 0.7500 0.7547  0.7829 0.8649    0
## 
## Kappa 
##           Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## rf      0.4577  0.5337 0.5801 0.5789  0.6347 0.6754    0
## rf2     0.4977  0.5337 0.5664 0.5791  0.6247 0.6837    0
## glmnet  0.5265  0.5833 0.5988 0.6156  0.6428 0.7965    0
## glmnet2 0.5260  0.5555 0.6254 0.6321  0.6758 0.7965    0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Plot the differences between model fits
dotplot(resampled, metric = &amp;quot;Accuracy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Ghosts_Goblins_Ghouls_01_files/figure-markdown_github/unnamed-chunk-29-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;predicting-creature-identity&#34;&gt;Predicting Creature Identity&lt;/h2&gt;

&lt;p&gt;Although I generated four models above, the second glmnet model (all interactions but without color) provided the highest accuracy, so I&amp;rsquo;ll use that model to predict survival in the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Reorder the data by creature ID number
test_complete &amp;lt;- test_complete %&amp;gt;%
                  arrange(id)

# Make predicted survival values
my_prediction &amp;lt;- predict(glm_model_2, test_complete)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;preparing-the-prediction-for-kaggle&#34;&gt;Preparing the prediction for Kaggle&lt;/h3&gt;

&lt;p&gt;The instructions on Kaggle indicate that they are expecting a csv file with 2 columns: ID and Creature Type. I need to make sure that my data are arranged properly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a data frame with two columns
my_solution_GGG_03 &amp;lt;- data.frame(id = test_complete$id, Type = my_prediction)

# Write the solution to a csv file 
write.csv(my_solution_GGG_03, file = &amp;quot;my_solution_GGG_03.csv&amp;quot;, row.names = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;testing-with-kaggle&#34;&gt;Testing with Kaggle&lt;/h3&gt;

&lt;p&gt;Looks like that submission scored 0.74669! Not bad!!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;d love to hear any feedback you may have on this process. Thanks in advance!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Get in touch</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/contact/</link>
      <pubDate>Sun, 06 Nov 2016 13:00:25 +0530</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/contact/</guid>
      <description>&lt;p&gt;When she reached the first hills of the Italic Mountains, she had a last view back on the skyline of her hometown Bookmarksgrove, the headline of Alphabet Village and the subline of her own road, the Line Lane. Pityful a rethoric question ran over her cheek, then&lt;/p&gt;

&lt;p&gt;Effects present letters inquiry no an removed or friends. Desire behind latter me though in. Supposing shameless am he engrossed up additions. My possible peculiar together to. Desire so better am cannot he up before points. Remember mistaken opinions it pleasure of debating. Court front maids forty if aware their at. Chicken use are pressed removed.&lt;/p&gt;

&lt;p&gt;Able an hope of body. Any nay shyness article matters own removal nothing his forming. Gay own additions education satisfied the perpetual. If he cause manor happy. Without farther she exposed saw man led. Along on happy could cease green oh.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/post/hello-world/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/post/hello-world/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;#####../content/post/hello-world_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/about/aboutme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/about/aboutme/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TL;DR&lt;/h3&gt;
&lt;p&gt;I’m a marine biologist and science communicator by training, but I’m taking all of the data analysis, statistics, and mathematical modeling skills I’ve learned and I’m now applying them to new problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-long-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Long Version&lt;/h3&gt;
&lt;p&gt;I’ve always loved science, exploration, and discovery. My natural curiosity, love of animals and the environment, and constant pursuit of knowledge led me into a marine biology and chemistry double major in college (BS/BA) and a Master’s degree in Marine Sciences. That generally leads people to think that I was “playing with animals” all day.&lt;/p&gt;
&lt;p&gt;And while I &lt;em&gt;did&lt;/em&gt; get to work with some amazing animals over the years…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/private/var/folders/pt/1zhlf3gd7pxf4hxnp9877z1c0000gp/T/RtmpR3wlaW/fileb03f68004396.jpeg&#34; width=&#34;655&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/about/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;div id=&#34;tldr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TL;DR&lt;/h3&gt;
&lt;p&gt;I’m a marine biologist and science communicator by training, but I’m taking all of the data analysis, statistics, and mathematical modeling skills I’ve learned (and continue to learn) and I’m now applying them to new problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-long-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Long Version&lt;/h3&gt;
&lt;p&gt;I’ve always loved science, exploration, and discovery. My natural curiosity, love of animals and the environment, and constant pursuit of knowledge led me into a marine biology and chemistry double major in college (BS/BA) and a Master’s degree in Marine Sciences. That generally leads people to think that I was “playing with animals” all day.&lt;/p&gt;
&lt;p&gt;And while I &lt;em&gt;did&lt;/em&gt; get to work with some amazing animals over the years…&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/all_animals.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Only about &lt;strong&gt;5 - 10%&lt;/strong&gt; of my time was spent working with animals. My professional experience (in time spent performing a single activity) breaks down like this:&lt;/p&gt;
&lt;/div&gt;



&lt;!-- BLOGDOWN-HEAD






/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Kaggle - Machine Learning from Disaster</title>
      <link>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Titanic_Survival/</link>
      <pubDate>Sat, 05 Nov 2016 18:25:22 +0530</pubDate>
      
      <guid>/Users/amberthomas1/Desktop/DataScience/Other/Blogdown/public/portfolio/Titanic_Survival/</guid>
      <description>&lt;p&gt;Data exploration and machine learning in RMarkdown.
&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is my first project on Kaggle and my first attempt at machine learning.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll do my best to illustrate what I&amp;rsquo;ve down and the logic behind my actions, but feedback is very much welcome and appreciated!&lt;/p&gt;

&lt;h3 id=&#34;loading-necessary-packages&#34;&gt;Loading Necessary Packages&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# For data manipulation and tidying
library(dplyr)

# For data visualizations
library(ggplot2)

# For modeling and predictions
library(caret)
library(glmnet)
library(ranger)
library(e1071)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;importing-data&#34;&gt;Importing Data&lt;/h3&gt;

&lt;p&gt;The data were downloaded directly from the &lt;a href=&#34;https://www.kaggle.com/c/titanic/data&#34;&gt;Kaggle Website&lt;/a&gt;. Before binding the training and test sets into a single data file, I added a column called &amp;ldquo;Dataset&amp;rdquo; and labelled rows from the training file &amp;ldquo;train&amp;rdquo; and rows from the testing file &amp;ldquo;test&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train &amp;lt;- read.csv(file = &amp;quot;train.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)
train$Dataset &amp;lt;- &amp;quot;train&amp;quot;

test &amp;lt;- read.csv(file = &amp;quot;test.csv&amp;quot;, header = TRUE, stringsAsFactors = FALSE)
test$Dataset &amp;lt;- &amp;quot;test&amp;quot;

full &amp;lt;- bind_rows(train, test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The full dataset can then be inspected:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(full)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## &#39;data.frame&#39;:    1309 obs. of  13 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : chr  &amp;quot;Braund, Mr. Owen Harris&amp;quot; &amp;quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&amp;quot; &amp;quot;Heikkinen, Miss. Laina&amp;quot; &amp;quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&amp;quot; ...
##  $ Sex        : chr  &amp;quot;male&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; &amp;quot;female&amp;quot; ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : chr  &amp;quot;A/5 21171&amp;quot; &amp;quot;PC 17599&amp;quot; &amp;quot;STON/O2. 3101282&amp;quot; &amp;quot;113803&amp;quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  &amp;quot;&amp;quot; &amp;quot;C85&amp;quot; &amp;quot;&amp;quot; &amp;quot;C123&amp;quot; ...
##  $ Embarked   : chr  &amp;quot;S&amp;quot; &amp;quot;C&amp;quot; &amp;quot;S&amp;quot; &amp;quot;S&amp;quot; ...
##  $ Dataset    : chr  &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; &amp;quot;train&amp;quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It appears that several of these variables should be represented as factors and thus should be reclassified.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;factor_variables &amp;lt;- c(&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Embarked&#39;, &#39;Dataset&#39;)
full[factor_variables] &amp;lt;- lapply(full[factor_variables], function(x) as.factor(x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now left with the following variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Passenger ID&lt;/strong&gt; : A seemingly unique number assigned to each passenger&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Survived&lt;/strong&gt; : A binary indicator of survival (0 = died, 1 = survived)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pclass&lt;/strong&gt; : A proxy for socio-economic status (1 = upper, 3 = lower)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Name&lt;/strong&gt; : Passenger&amp;rsquo;s Name. For wedded women, her husband&amp;rsquo;s name appears first and her maiden name appears in parentheses&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sex&lt;/strong&gt; : General indication of passenger&amp;rsquo;s sex&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Age&lt;/strong&gt; : Age of passenger (or approximate age). Passengers under the age of 1 year have fractional ages&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;SibSp&lt;/strong&gt; : A count of the passenger&amp;rsquo;s siblings or spouses aboard&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Parch&lt;/strong&gt; : A count of the passenger&amp;rsquo;s parents or siblings aboard&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ticket&lt;/strong&gt; : The number printed on the ticket. The numbering system is not immediately apparent&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fare&lt;/strong&gt; : The price for the ticket (presumably in pounds, shillings, and pennies)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cabin&lt;/strong&gt; : Cabin number occupied by the passenger (this field is quite empty)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Embarked&lt;/strong&gt; : The port from which the passenger boarded the ship&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; : Whether this particular row was a part of the training or testing dataset&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;feature-engineering&#34;&gt;Feature Engineering&lt;/h2&gt;

&lt;h3 id=&#34;names-and-titles&#34;&gt;Names and Titles&lt;/h3&gt;

&lt;p&gt;At first glance, the &amp;ldquo;Name&amp;rdquo; column doesn&amp;rsquo;t help too much as there are 1307 unique names, however, this column also includes embedded title information that may be of interest. I decided to use &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&#34;&gt;regular expressions&lt;/a&gt; and the &lt;code&gt;gsub()&lt;/code&gt; functions to extract the titles into a new variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names &amp;lt;- full$Name

titles &amp;lt;-  gsub(&amp;quot;^.*, (.*?)\\..*$&amp;quot;, &amp;quot;\\1&amp;quot;, names)

full$Titles &amp;lt;- titles

unique(full$Titles)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Mr&amp;quot;           &amp;quot;Mrs&amp;quot;          &amp;quot;Miss&amp;quot;         &amp;quot;Master&amp;quot;      
##  [5] &amp;quot;Don&amp;quot;          &amp;quot;Rev&amp;quot;          &amp;quot;Dr&amp;quot;           &amp;quot;Mme&amp;quot;         
##  [9] &amp;quot;Ms&amp;quot;           &amp;quot;Major&amp;quot;        &amp;quot;Lady&amp;quot;         &amp;quot;Sir&amp;quot;         
## [13] &amp;quot;Mlle&amp;quot;         &amp;quot;Col&amp;quot;          &amp;quot;Capt&amp;quot;         &amp;quot;the Countess&amp;quot;
## [17] &amp;quot;Jonkheer&amp;quot;     &amp;quot;Dona&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a bit more manageable: only 18 unique titles. Time to see how many times each title was used. I decided to make a table separated by sex.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(full$Sex, full$Title)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##         
##          Capt Col Don Dona  Dr Jonkheer Lady Major Master Miss Mlle Mme
##   female    0   0   0    1   1        0    1     0      0  260    2   1
##   male      1   4   1    0   7        1    0     2     61    0    0   0
##         
##           Mr Mrs  Ms Rev Sir the Countess
##   female   0 197   2   0   0            1
##   male   757   0   0   8   1            0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It looks like Captain, Don, Dona, Jonkheer, Lady, Madame, Sir and the Countess were each only used once. I&amp;rsquo;ll leave Captain separate, but the rest should be combined with similar categories.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Don&lt;/strong&gt; : A Spanish/Portuguese/Italian title used with, but not instead of, a name.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dona&lt;/strong&gt; : Female version of &amp;ldquo;Don&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jonkheer&lt;/strong&gt; : Dutch honorific of nobility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lady&lt;/strong&gt; : English honorific of nobility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Madame&lt;/strong&gt; : French, polite form of address for a woman&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sir&lt;/strong&gt; : Honorific address (male)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the Countess&lt;/strong&gt; : Rank of nobility (female)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It seems that most of the rarely used titles indicate some form of nobility. That&amp;rsquo;s easy to check with another table comparing &lt;code&gt;Pclass&lt;/code&gt; and &lt;code&gt;Titles&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;table(full$Pclass, full$Titles)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    
##     Capt Col Don Dona  Dr Jonkheer Lady Major Master Miss Mlle Mme  Mr Mrs
##   1    1   4   1    1   6        1    1     2      5   60    2   1 159  77
##   2    0   0   0    0   2        0    0     0     11   50    0   0 150  55
##   3    0   0   0    0   0        0    0     0     45  150    0   0 448  65
##    
##      Ms Rev Sir the Countess
##   1   0   0   1            1
##   2   1   8   0            0
##   3   1   0   0            0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since Don, Jonkheer, and Sir are all of similar usage, and each represent only one first-class man, I combined them into the category &amp;ldquo;Sir&amp;rdquo;. Dona, Lady, Madame, and the Countess each only represent one first-class woman, so I combined them into the category &amp;ldquo;Lady&amp;rdquo;. These values were substituted using the &lt;code&gt;gsub&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full$Titles &amp;lt;- gsub(&amp;quot;Dona|Lady|Madame|the Countess&amp;quot;, &amp;quot;Lady&amp;quot;, full$Titles)
full$Titles &amp;lt;- gsub(&amp;quot;Don|Jonkheer|Sir&amp;quot;, &amp;quot;Sir&amp;quot;, full$Titles)

unique(full$Titles)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Mr&amp;quot;     &amp;quot;Mrs&amp;quot;    &amp;quot;Miss&amp;quot;   &amp;quot;Master&amp;quot; &amp;quot;Sir&amp;quot;    &amp;quot;Rev&amp;quot;    &amp;quot;Dr&amp;quot;    
##  [8] &amp;quot;Mme&amp;quot;    &amp;quot;Ms&amp;quot;     &amp;quot;Major&amp;quot;  &amp;quot;Lady&amp;quot;   &amp;quot;Mlle&amp;quot;   &amp;quot;Col&amp;quot;    &amp;quot;Capt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: If you are planning to replicate the above substitution without any RegEx, make sure that you substitute &amp;ldquo;Dona&amp;rdquo; before substituting &amp;ldquo;Don&amp;rdquo;! Otherwise, &amp;ldquo;Dona&amp;rdquo; becomes &amp;ldquo;Sira&amp;rdquo; (as the &amp;ldquo;Don&amp;rdquo; part was replaced with &amp;ldquo;Sir&amp;rdquo;) and your second substitution won&amp;rsquo;t find or replace &amp;ldquo;Dona&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Lastly for the titles, they should be factors, not character strings.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full$Titles &amp;lt;- as.factor(full$Titles)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These titles could certainly be condensed more, but for the time being, I am going to leave them separated as is.&lt;/p&gt;

&lt;p&gt;I have some thoughts about wanting to split up the names further to find family groups, but since many familial relationships (cousins, nieces/nephews, aunts/uncles, fiances, mistresses, in-laws, children with a nanny or close friends) aren&amp;rsquo;t reported in any way in this data set, I&amp;rsquo;ll have to think a little longer about the most appropriate way to find actual family groups.&lt;/p&gt;

&lt;h3 id=&#34;sibsp-and-parch-for-family-size&#34;&gt;SibSp and Parch for Family Size&lt;/h3&gt;

&lt;p&gt;Since the SibSp and Parch variables each give some indication as to close family members that were also aboard the ship, it would make sense to calculate family size as a combination of SibSp, Parch and the passenger in question.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full &amp;lt;- mutate(full, FamilySize = SibSp + Parch + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s visualize family size&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-11-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Lots of people without immediate family with them. Perhaps these people were traveling with other family members/friends that weren&amp;rsquo;t captured in the SibSp / Parch variables.&lt;/p&gt;

&lt;h3 id=&#34;ticket-numbers-and-travel-groups&#34;&gt;Ticket Numbers and Travel Groups&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve decided that another possible way to discern groups that were travelling together is to look at the ticket numbers. It appears that families or groups who purchased their tickets together have identical ticket numbers, thus quantifying the number of families or traveling groups. A quick look at the unique ticket numbers indicates there are 929 of them in the full data set (out of a possible 1309 passengers).&lt;/p&gt;

&lt;p&gt;It seems the easiest way to separate these tickets is to create a new column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full$TravelGroup &amp;lt;- NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then arrange the data by ticket number using the &lt;code&gt;arrange()&lt;/code&gt; function from the &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full2 &amp;lt;- arrange(full, Ticket)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take a look at the first few rows of results&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(full2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   PassengerId Survived Pclass
## 1         258        1      1
## 2         505        1      1
## 3         760        1      1
## 4         263        0      1
## 5         559        1      1
## 6         586        1      1
##                                                       Name    Sex Age
## 1                                     Cherry, Miss. Gladys female  30
## 2                                    Maioni, Miss. Roberta female  16
## 3 Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards) female  33
## 4                                        Taussig, Mr. Emil   male  52
## 5                   Taussig, Mrs. Emil (Tillie Mandelbaum) female  39
## 6                                      Taussig, Miss. Ruth female  18
##   SibSp Parch Ticket  Fare Cabin Embarked Dataset Titles FamilySize
## 1     0     0 110152 86.50   B77        S   train   Miss          1
## 2     0     0 110152 86.50   B79        S   train   Miss          1
## 3     0     0 110152 86.50   B77        S   train   Lady          1
## 4     1     1 110413 79.65   E67        S   train     Mr          3
## 5     1     1 110413 79.65   E67        S   train    Mrs          3
## 6     0     2 110413 79.65   E68        S   train   Miss          3
##   TravelGroup
## 1          NA
## 2          NA
## 3          NA
## 4          NA
## 5          NA
## 6          NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To verify that this is working so far, I inspected the first ticket number listed (110152) on the &lt;a href=&#34;https://www.encyclopedia-titanica.org/titanic-passengers-and-crew/&#34;&gt;Titanic Passenger and Crew&lt;/a&gt; table of Encyclopedia Titanica. That dataset lists the same three passengers owned those tickets, verified that the 3 women were traveling together, and indicated that two of the women (Miss Gladys Cherry and the Countess of Rothes) were cousins and the 3rd woman in their party (Miss Roberta Elizabeth Mary Maioni) was their servant. Looking at unique Ticket ID may be the only way to know that these women were travelling together. I&amp;rsquo;m feeling good that unique ticket numbers may be a good way to look at family/traveling groups, so full steam ahead!&lt;/p&gt;

&lt;p&gt;Next, I need to generate a &amp;ldquo;TravelGroup&amp;rdquo; number. To do this, I will use the &lt;code&gt;transform&lt;/code&gt; function looking for matching unique Ticket numbers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full2 &amp;lt;- (transform(full2, TravelGroup = match(Ticket, unique(Ticket))))

# Can&#39;t forget to make those Travel Groups into factors!
full2$TravelGroup &amp;lt;- as.factor(full2$TravelGroup)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This generates 929 unique Travel Groups, which is the same number of unique Ticket numbers. So far so good.&lt;/p&gt;

&lt;p&gt;It may also be of interest to look at group size. We can generate this using the &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;mutate&lt;/code&gt; functions in &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full3 &amp;lt;- full2 %&amp;gt;% 
            group_by(TravelGroup) %&amp;gt;% 
            mutate(GroupSize = n()) %&amp;gt;%
            ungroup()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How does Travel Group Size compare to Family Group Size that we calculated earlier?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-17-1.png&#34; alt=&#34;&#34; /&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-17-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;They look pretty close, again showing that most people were potentially travelling alone.&lt;/p&gt;

&lt;p&gt;Now to check if those with the unique Ticket IDs were really travelling alone:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;filtered &amp;lt;- filter(full3, GroupSize == 1)

# How many were listed as being onboard with siblings or spouses?
fSibSp &amp;lt;- filtered[filtered$SibSp &amp;gt; 0, ]
nrow(fSibSp)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 42
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# How many were listed as being onboard with parents or children?
fParch &amp;lt;- filtered[filtered$Parch &amp;gt; 0, ]
nrow(fParch)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 16
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# How many of those people overlapped both groups?
sum(fSibSp$PassengerId %in% fParch$PassengerId)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oops! Looks like we were counting 50 passengers as solo-riders when they were actually riding with family. Given the current information, I&amp;rsquo;m not sure how to know to tell who was travelling together. Manually summing SibSp and Parch to estimate family size doesn&amp;rsquo;t account for other types of groups that were travelling together and looking only at unique Ticket Number doesn&amp;rsquo;t account for some travelling with family who purchased a separate ticket. I could override the GroupSize for those 50 that weren&amp;rsquo;t actually riding solo, but their TravelGroup number won&amp;rsquo;t be accurate. For the time being, I&amp;rsquo;m going to leave TravelGroup and GroupSize as is.&lt;/p&gt;

&lt;h2 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h2&gt;

&lt;p&gt;At this point, I&amp;rsquo;m feeling pretty good about the Feature Engineering that I&amp;rsquo;ve done so far. Time to correct for missing data!&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at what has NA values:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(full3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   PassengerId   Survived   Pclass      Name               Sex     
##  1      :   1   0   :549   1:323   Length:1309        female:466  
##  2      :   1   1   :342   2:277   Class :character   male  :843  
##  3      :   1   NA&#39;s:418   3:709   Mode  :character               
##  4      :   1                                                     
##  5      :   1                                                     
##  6      :   1                                                     
##  (Other):1303                                                     
##       Age            SibSp            Parch          Ticket         
##  Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       
##  1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  
##  Median :28.00   Median :0.0000   Median :0.000   Mode  :character  
##  Mean   :29.88   Mean   :0.4989   Mean   :0.385                     
##  3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     
##  Max.   :80.00   Max.   :8.0000   Max.   :9.000                     
##  NA&#39;s   :263                                                        
##       Fare            Cabin           Embarked  Dataset        Titles   
##  Min.   :  0.000   Length:1309         :  2    test :418   Mr     :757  
##  1st Qu.:  7.896   Class :character   C:270    train:891   Miss   :260  
##  Median : 14.454   Mode  :character   Q:123                Mrs    :197  
##  Mean   : 33.295                      S:914                Master : 61  
##  3rd Qu.: 31.275                                           Dr     :  8  
##  Max.   :512.329                                           Rev    :  8  
##  NA&#39;s   :1                                                 (Other): 18  
##    FamilySize      TravelGroup     GroupSize     
##  Min.   : 1.000   779    :  11   Min.   : 1.000  
##  1st Qu.: 1.000   105    :   8   1st Qu.: 1.000  
##  Median : 1.000   776    :   8   Median : 1.000  
##  Mean   : 1.884   336    :   7   Mean   : 2.102  
##  3rd Qu.: 2.000   455    :   7   3rd Qu.: 3.000  
##  Max.   :11.000   460    :   7   Max.   :11.000  
##                   (Other):1261
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like we are missing values in the &amp;ldquo;Survived&amp;rdquo; variable (which is to be expected since this is a combination of the training and test datasets), &amp;ldquo;Fare&amp;rdquo;, &amp;ldquo;Embarked&amp;rdquo;, and quite a few in the &amp;ldquo;Age&amp;rdquo; column. We&amp;rsquo;ll start with &amp;ldquo;Fare&amp;rdquo;.&lt;/p&gt;

&lt;h3 id=&#34;missing-fare&#34;&gt;Missing Fare&lt;/h3&gt;

&lt;p&gt;Which passenger has no fare information?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full3[(which(is.na(full3$Fare))) , 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##   PassengerId
##        &amp;lt;fctr&amp;gt;
## 1        1044
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like Passenger number 1044 has no listed Fare.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Resort the dataset by Passenger Number
full4 &amp;lt;- arrange(full3, PassengerId)

# Where did this passenger leave from? What was their class?
full4[1044, c(3, 12)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   Pclass Embarked
##   &amp;lt;fctr&amp;gt;   &amp;lt;fctr&amp;gt;
## 1      3        S
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like he left from &amp;rsquo;S&amp;rsquo; (Southampton) as a 3rd class passenger. Let&amp;rsquo;s see what other people of the same class and embarkment port paid for their tickets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-22-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4 %&amp;gt;%
  filter(Pclass == &#39;3&#39; &amp;amp; Embarked == &#39;S&#39;) %&amp;gt;%
  summarise(missing_fare = median(Fare, na.rm = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 1
##   missing_fare
##          &amp;lt;dbl&amp;gt;
## 1         8.05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like the median cost for a 3rd class passenger leaving out of Southampton was 8.05. That seems like a logical value for this passenger to have paid.&lt;/p&gt;

&lt;p&gt;Time to replace that NA with 8.05&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4$Fare[1044] &amp;lt;- 8.05

summary(full4$Fare)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   7.896  14.450  33.280  31.280 512.300
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hooray! No more NA values for Fare.&lt;/p&gt;

&lt;h3 id=&#34;missing-embarkment&#34;&gt;Missing Embarkment&lt;/h3&gt;

&lt;p&gt;Which passengers have no listed embarkment port?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4$Embarked[full4$Embarked == &amp;quot;&amp;quot;] &amp;lt;- NA

full4[(which(is.na(full4$Embarked))), 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 1
##   PassengerId
##        &amp;lt;fctr&amp;gt;
## 1          62
## 2         830
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, so Passenger numbers 62 and 830 are each missing their embarkment ports. Let&amp;rsquo;s look at their class of ticket and their fare.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4[c(62, 830), c(1,3,10)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
##   PassengerId Pclass  Fare
##        &amp;lt;fctr&amp;gt; &amp;lt;fctr&amp;gt; &amp;lt;dbl&amp;gt;
## 1          62      1    80
## 2         830      1    80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both passengers had first class tickets that they spent 80 (pounds?) on. Let&amp;rsquo;s see the embarkment ports of others who bought similar kinds of tickets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4 %&amp;gt;%
  group_by(Embarked, Pclass) %&amp;gt;%
  filter(Pclass == &amp;quot;1&amp;quot;) %&amp;gt;%
  summarise(mfare = median(Fare),
            n = n())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Source: local data frame [4 x 4]
## Groups: Embarked [?]
## 
##   Embarked Pclass   mfare     n
##     &amp;lt;fctr&amp;gt; &amp;lt;fctr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
## 1        C      1 76.7292   141
## 2        Q      1 90.0000     3
## 3        S      1 52.0000   177
## 4       NA      1 80.0000     2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like the median price for a first class ticket departing from &amp;lsquo;C&amp;rsquo; (Charbourg) was 77 (in comparison to our 80). While first class tickets departing from &amp;lsquo;Q&amp;rsquo; were only slightly more expensive (median price 90), only 3 first class passengers departed from that port. It seems far more likely that passengers 62 and 830 departed with the other 141 first-class passengers from Charbourg.&lt;/p&gt;

&lt;p&gt;Now to replace their NA values with &amp;lsquo;C&amp;rsquo;. And drop any unused levels.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Assign empty embark ports to &#39;C&#39;
full4$Embarked[c(62,830)] &amp;lt;- &#39;C&#39;

# Drop unused levels (since there should be no more blanks)
full4$Embarked &amp;lt;- droplevels(full4$Embarked)

# Check to make sure there are no NA&#39;s or blanks
levels(full4$Embarked)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;C&amp;quot; &amp;quot;Q&amp;quot; &amp;quot;S&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yay! No more NA values for Embarked.&lt;/p&gt;

&lt;h3 id=&#34;missing-age&#34;&gt;Missing Age&lt;/h3&gt;

&lt;p&gt;This one is a bit trickier. 263 passengers have no age listed. Taking a median age of all passengers doesn&amp;rsquo;t seem like the best way to solve this problem, so it may be easiest to try to predict the passengers&amp;rsquo; age based on other known information.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve decided to use the &lt;code&gt;caret&lt;/code&gt; package for predicting age.&lt;/p&gt;

&lt;p&gt;Generate a random forest model on the full dataset (minus the age values that are NA)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predicted_age &amp;lt;- train(
  Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + GroupSize,
  tuneGrid = data.frame(mtry = c(2, 3, 7)),
  data = full4[!is.na(full4$Age), ],
  method = &amp;quot;ranger&amp;quot;,
  trControl = trainControl(
      method = &amp;quot;cv&amp;quot;, number = 10,
      repeats = 10, verboseIter = TRUE),
  importance = &#39;impurity&#39;
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s look at what factors were the most important in modeling age:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-30-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Wow! Looks like it was a good idea to split out Titles!&lt;/p&gt;

&lt;p&gt;Now to use this information to predict the ages of passengers with missing ages and filling in their NA values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;full4$Age[is.na(full4$Age)] &amp;lt;- predict(predicted_age, full4[is.na(full4$Age),])

# Check the summary to make sure there are no more NA values
summary(full4$Age)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.17   22.00   28.44   29.74   37.00   80.00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s take a quick look at the age distribution of passengers with originally known ages, and the age distribution of the entire group (known and predicted ages) to make sure we didn&amp;rsquo;t terribly skew the distribution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-32-1.png&#34; alt=&#34;&#34; /&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-32-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hmm, seems to have shifted a bit, but that could be due to a greater lack of age information collected for middle-aged passengers.&lt;/p&gt;

&lt;h2 id=&#34;modeling-for-survival&#34;&gt;Modeling for Survival&lt;/h2&gt;

&lt;p&gt;First things first, I need to split out the test and training data back into separate data sets, now called &lt;code&gt;train_complete&lt;/code&gt; and &lt;code&gt;test_complete&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train_complete &amp;lt;- full4[full4$Dataset == &#39;train&#39;, ]
test_complete &amp;lt;- full4[full4$Dataset == &#39;test&#39;, ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because I plan on using the &lt;code&gt;caret&lt;/code&gt; package for all of my modeling, I&amp;rsquo;m going to generate a standard &lt;code&gt;trainControl&lt;/code&gt; so that those tuning parameters remain consistent throughout the various models.&lt;/p&gt;

&lt;h3 id=&#34;creating-traincontrol&#34;&gt;Creating trainControl&lt;/h3&gt;

&lt;p&gt;I will create a system that will perform 10 repeats of a 10-Fold cross-validation of the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;myControl &amp;lt;- trainControl(
      method = &amp;quot;cv&amp;quot;, 
      number = 10,
      repeats = 10, 
      verboseIter = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fitting-a-random-forest-model&#34;&gt;Fitting a random forest model&lt;/h3&gt;

&lt;p&gt;The first type of model I&amp;rsquo;d like to use is a random forest model (using the &lt;code&gt;ranger&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt; packages).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rf_model &amp;lt;- train(
    Survived ~ Age + Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + 
      TravelGroup + GroupSize,
    tuneGrid = data.frame(mtry = c(2, 5, 8, 10, 15)),
    data = train_complete, 
    method = &amp;quot;ranger&amp;quot;, 
    trControl = myControl,
    importance = &#39;impurity&#39;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;fitting-a-glmnet-model&#34;&gt;Fitting a glmnet model&lt;/h3&gt;

&lt;p&gt;Next, we&amp;rsquo;ll try a glmnet model, also from the &lt;code&gt;caret&lt;/code&gt; package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;glm_model &amp;lt;- train(
    Survived ~ Age + Pclass + Sex + SibSp + Parch + Fare + Embarked + Titles + FamilySize + 
      TravelGroup + GroupSize, 
    method = &amp;quot;glmnet&amp;quot;,
    tuneGrid = expand.grid(alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)),
    data = train_complete,
    trControl = myControl
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;comparing-model-fit&#34;&gt;Comparing model fit&lt;/h3&gt;

&lt;p&gt;Now that we have a random forest model and a glmnet model, it&amp;rsquo;s time to compare their fit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a list of models
models &amp;lt;- list(rf = rf_model, glmnet = glm_model)

# Resample the models
resampled &amp;lt;- resamples(models)

# Generate a summary
summary(resampled)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## summary.resamples(object = resampled)
## 
## Models: rf, glmnet 
## Number of resamples: 10 
## 
## Accuracy 
##          Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## rf     0.7865  0.8022 0.8146 0.8249  0.8507 0.8667    0
## glmnet 0.8090  0.8357 0.8492 0.8486  0.8624 0.8977    0
## 
## Kappa 
##          Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## rf     0.5348  0.5860 0.6041 0.6237  0.6699 0.7165    0
## glmnet 0.5838  0.6435 0.6761 0.6753  0.7056 0.7807    0
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Plot the differences between model fits
dotplot(resampled, metric = &amp;quot;Accuracy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;Titanic_Survival_R_Markdown_3_files/figure-markdown_github/unnamed-chunk-37-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks like the glmnet model is slightly more accurate than the random forest model, so we&amp;rsquo;ll use that to predict the survival rate.&lt;/p&gt;

&lt;p&gt;Ok, time to make some predictions.&lt;/p&gt;

&lt;h2 id=&#34;predicting-survival&#34;&gt;Predicting Survival&lt;/h2&gt;

&lt;p&gt;Although I generated two models above, the glmnet model provided higher accuracy, so I&amp;rsquo;ll use that model to predict survival in the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Reorder the data by Passenger ID number
test_complete &amp;lt;- test_complete %&amp;gt;%
                  arrange(PassengerId)

# Make predicted survival values
my_prediction &amp;lt;- predict(glm_model, test_complete)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;preparing-the-prediction-for-kaggle&#34;&gt;Preparing the prediction for Kaggle&lt;/h3&gt;

&lt;p&gt;The instructions on Kaggle indicate that they are expecting a csv file with 2 columns: Passenger ID and Survived. I need to make sure that my data are arranged properly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Create a data frame with two columns: PassengerId &amp;amp; Survived where Survived contains my predictions.
my_solution_5 &amp;lt;- data.frame(PassengerID = test$PassengerId, Survived = my_prediction)

# Write the solution to a csv file 
write.csv(my_solution_5, file = &amp;quot;my_solution_5.csv&amp;quot;, row.names = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;testing-with-kaggle&#34;&gt;Testing with Kaggle&lt;/h3&gt;

&lt;p&gt;Looks like that submission scored 0.80383! Not bad!!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;d love to hear any feedback you may have on this process. Thanks in advance!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>